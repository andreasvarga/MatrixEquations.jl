var documenterSearchIndex = {"docs":
[{"location":"meutil.html#Matrix-Equations-Utilities","page":"Matrix Equations Utilities","title":"Matrix Equations Utilities","text":"","category":"section"},{"location":"meutil.html","page":"Matrix Equations Utilities","title":"Matrix Equations Utilities","text":"utqu\nutqu!\nqrupdate!\nrqupdate!\nisschur\ntriu2vec\nvec2triu","category":"page"},{"location":"meutil.html#MatrixEquations.utqu","page":"Matrix Equations Utilities","title":"MatrixEquations.utqu","text":"X = utqu(Q,U)\n\nCompute efficiently the symmetric/hermitian product X = U'QU, where Q is a symmetric/hermitian matrix.\n\n\n\n\n\n","category":"function"},{"location":"meutil.html#MatrixEquations.utqu!","page":"Matrix Equations Utilities","title":"MatrixEquations.utqu!","text":"utqu!(Q,U) -> Q\n\nCompute efficiently the symmetric/hermitian product U'QU -> Q, where Q is a symmetric/hermitian matrix and U is a square matrix. The resulting product overwrites Q.\n\n\n\n\n\n","category":"function"},{"location":"meutil.html#MatrixEquations.qrupdate!","page":"Matrix Equations Utilities","title":"MatrixEquations.qrupdate!","text":"qrupdate!(R, Y) -> R\n\nUpdate the upper triangular factor R by the upper triangular factor of the QR factorization  of [ R; Y' ], where Y is a low-rank matrix Y (typically with one or two columns). The computation of R only uses O(n^2) operations (n is the size of R). The input matrix R is updated in place and the matrix Y is destroyed during the computation.\n\n\n\n\n\n","category":"function"},{"location":"meutil.html#MatrixEquations.rqupdate!","page":"Matrix Equations Utilities","title":"MatrixEquations.rqupdate!","text":"rqupdate!(R, Y) -> R\n\nUpdate the upper triangular factor R by the upper triangular factor of the RQ factorization  of [ Y R], where Y is a low-rank matrix Y (typically with one or two columns). The computation of R only uses O(n^2) operations (n is the size of R). The input matrix R is updated in place and the matrix Y is destroyed during the computation.\n\n\n\n\n\n","category":"function"},{"location":"meutil.html#MatrixEquations.isschur","page":"Matrix Equations Utilities","title":"MatrixEquations.isschur","text":" isschur(A::AbstractMatrix) -> Bool\n\nTest whether A is a square matrix in a real or complex Schur form. In the real case, it is only tested whether A is a quasi upper triangular matrix, which may have 2x2 diagonal blocks, which however must not correspond to complex conjugate eigenvalues. In the complex case, it is tested if A is upper triangular.\n\n\n\n\n\n isschur(A::AbstractMatrix, B::AbstractMatrix) -> Bool\n\nTest whether (A,B) is a pair of square matrices in a real or complex generalized Schur form. In the real case, it is only tested whether B is upper triangular and A is a quasi upper triangular matrix, which may have 2x2 diagonal blocks, which however must not correspond to complex conjugate eigenvalues. In the complex case, it is tested if A and B are both upper triangular.\n\n\n\n\n\n","category":"function"},{"location":"meutil.html#MatrixEquations.triu2vec","page":"Matrix Equations Utilities","title":"MatrixEquations.triu2vec","text":"x = triu2vec(Q; rowwise = false, her = false)\n\nReshape the upper triangular part of the nxn array Q as a one-dimensional column vector x with n(n+1)/2 elements. Q is assumed symmetric/hermitian if her = true. The elements of x correspond to stacking the elements of successive columns of the upper triangular part of Q, if rowwise = false, or stacking the elements of successive rows of the upper triangular part of Q, if rowwise = true.\n\n\n\n\n\n","category":"function"},{"location":"meutil.html#MatrixEquations.vec2triu","page":"Matrix Equations Utilities","title":"MatrixEquations.vec2triu","text":"Q = vec2triu(x; rowwise = false, her = false)\n\nBuild from a one-dimensional column vector x with n(n+1)/2 elements an nxn upper triangular matrix Q if her = false or an nxn symmetric/hermitian array Q if her = true. The elements of x correspond to stacking the elements of successive columns of the upper triangular part of Q, if rowwise = false, or stacking the elements of successive rows of the upper triangular part of Q, if rowwise = true.\n\n\n\n\n\n","category":"function"},{"location":"riccati.html#Riccati-Matrix-Equation-Solvers","page":"Riccati Matrix Equation Solvers","title":"Riccati Matrix Equation Solvers","text":"","category":"section"},{"location":"riccati.html#Standard-Riccati-Matrix-Equations","page":"Riccati Matrix Equation Solvers","title":"Standard Riccati Matrix Equations","text":"","category":"section"},{"location":"riccati.html","page":"Riccati Matrix Equation Solvers","title":"Riccati Matrix Equation Solvers","text":"arec\nared","category":"page"},{"location":"riccati.html#MatrixEquations.arec","page":"Riccati Matrix Equation Solvers","title":"MatrixEquations.arec","text":"arec(A, G, Q = 0; scaling = 'B', pow2 = false, as = false, rtol::Real = nϵ) -> (X, EVALS, Z, scalinfo)\n\nCompute X, the hermitian/symmetric stabilizing solution (if as = false) or anti-stabilizing solution (if as = true) of the continuous-time algebraic Riccati equation\n\n A'X + XA - XGX + Q = 0,\n\nwhere G and Q are hermitian/symmetric matrices or uniform scaling operators. Scalar-valued G and Q are interpreted as appropriately sized uniform scaling operators G*I and Q*I. The Schur method of [1] is used. \n\nTo enhance the accuracy of computations, a block scaling of matrices G and Q is performed, if   the default setting scaling = 'B' is used. This scaling is however performed only if norm(Q) > norm(G). A general, eigenvalue computation oriented scaling combined with a block scaling is used if scaling = 'G' is selected.  An alternative, experimental structure preserving scaling can be performed using the option scaling = 'S'.  Scaling can be disabled with the choice scaling = 'N'. If pow2 = true, the scaling elements are enforced to the nearest power of 2 (default: pow2 = false).\n\nBy default, the lower bound for the 1-norm reciprocal condition number rtol is n*ϵ, where n is the order of A and ϵ is the machine epsilon of the element type of A.\n\nEVALS is a vector containing the (stable or anti-stable) eigenvalues of A-GX.\n\nZ = [U; V] is an orthogonal basis for the stable/anti-stable deflating subspace such that X = Sx*(V/U)*Sxi,  where Sx and Sxi are diagonal scaling matrices contained in the named tuple scalinfo  as scalinfo.Sx and scalinfo.Sxi, respectively.\n\nNote: To solve the continuous-time algebraic Riccati equation\n\n A'X + XA - XBR^(-1)B'X + Q = 0,\n\nwith R a hermitian/symmetric matrix and B a compatible size matrix, G = BR^(-1)B' must be provided.  This approach is not numerically suited when R is ill-conditioned and/or B has large norm.  \n\nReference:\n\n[1] Laub, A.J., A Schur Method for Solving Algebraic Riccati equations.     IEEE Trans. Auto. Contr., AC-24, pp. 913-921, 1979.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [-6. -2. 1.; 5. 1. -1; -4. -2. -1.]\n3×3 Array{Float64,2}:\n -6.0  -2.0   1.0\n  5.0   1.0  -1.0\n -4.0  -2.0  -1.0\n\njulia> G = [1. 0. 0.; 0. 5. 0.; 0. 0. 10.]\n3×3 Array{Float64,2}:\n 1.0  0.0   0.0\n 0.0  5.0   0.0\n 0.0  0.0  10.0\n\njulia> X, CLSEIG = arec(A,G,2I);\n\njulia> X\n3×3 Array{Float64,2}:\n  0.459589   0.333603   -0.144406\n  0.333603   0.65916    -0.0999216\n -0.144406  -0.0999216   0.340483\n\njulia> A'*X+X*A-X*G*X+2I\n3×3 Array{Float64,2}:\n  2.22045e-16  4.44089e-16  -1.77636e-15\n  4.44089e-16  6.66134e-16   1.11022e-16\n -1.77636e-15  1.11022e-16  -1.33227e-15\n\njulia> CLSEIG\n3-element Array{Complex{Float64},1}:\n -4.411547592296008 + 2.4222082620381102im\n -4.411547592296008 - 2.4222082620381102im\n -4.337128244724371 + 0.0im\n\njulia> eigvals(A-G*X)\n3-element Array{Complex{Float64},1}:\n -4.4115475922960075 - 2.4222082620381076im\n -4.4115475922960075 + 2.4222082620381076im\n  -4.337128244724374 + 0.0im\n\n\n\n\n\narec(A, B, R, Q, S; scaling = 'B', pow2 = false, as = false, rtol::Real = nϵ, orth = false) -> (X, EVALS, F, Z, scalinfo)\n\nCompute X, the hermitian/symmetric stabilizing solution (if as = false) or anti-stabilizing solution (if as = true) of the continuous-time algebraic Riccati equation\n\n A'X + XA - (XB+S)R^(-1)(B'X+S') + Q = 0,\n\nwhere R and Q are hermitian/symmetric matrices or uniform scaling operators such that R is nonsingular. Scalar-valued R and Q are interpreted as appropriately sized uniform scaling operators R*I and Q*I. S, if not specified, is set to S = zeros(size(B)). The Schur method of [1] is used. \n\nTo enhance the accuracy of computations, a block scaling of matrices G and Q is performed, if   the default setting scaling = 'B' is used. This scaling is however performed only if norm(Q) > norm(B)^2/norm(R). A general, eigenvalue computation oriented scaling combined with a block scaling is used if scaling = 'G' is selected.  An alternative, experimental structure preserving scaling can be performed using the option scaling = 'S'.  Similar, experimental structure preserving scalings can be performed using the options scaling = 'D'  or scaling = 'T'. Scaling can be disabled with the choice scaling = 'N'. If pow2 = true, the scaling elements are enforced to the nearest power of 2 (default: pow2 = false).\n\nBy default, the lower bound for the 1-norm reciprocal condition number rtol is n*ϵ, where n is the order of A and ϵ is the machine epsilon of the element type of A.\n\nEVALS is a vector containing the (stable or anti-stable) eigenvalues of A-BF.\n\nF is the stabilizing or anti-stabilizing gain matrix F = R^(-1)(B'X+S').\n\nZ = [U; V; W] is a basis for the relevant stable/anti-stable deflating subspace  such that X = Sx*(V/U)*Sxi and  F = -Sr*(W/U)*Sxi,  where Sx, Sxi and Sr are diagonal scaling matrices contained in the named tuple scalinfo  as scalinfo.Sx, scalinfo.Sxi and scalinfo.Sr, respectively. An orthogonal basis Z can be determined, with an increased computational cost, by setting orth = true.\n\nReference:\n\n[1] Laub, A.J., A Schur Method for Solving Algebraic Riccati equations.     IEEE Trans. Auto. Contr., AC-24, pp. 913-921, 1979.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [-6. -2. 1.; 5. 1. -1; -4. -2. -1.]\n3×3 Array{Float64,2}:\n -6.0  -2.0   1.0\n  5.0   1.0  -1.0\n -4.0  -2.0  -1.0\n\njulia> B = [1. 2.; 2. 0.; 0. 1.]\n3×2 Array{Float64,2}:\n 1.0  2.0\n 2.0  0.0\n 0.0  1.0\n\njulia> R = [1. 0.; 0. 5.]\n2×2 Array{Float64,2}:\n 1.0  0.0\n 0.0  5.0\n\njulia> X, CLSEIG, F = arec(A,B,R,2I);\n\njulia> X\n3×3 Array{Float64,2}:\n  0.522588   0.303007  -0.327227\n  0.303007   0.650895  -0.132608\n -0.327227  -0.132608   0.629825\n\njulia> A'*X+X*A-X*B*inv(R)*B'*X+2I\n3×3 Array{Float64,2}:\n -2.66454e-15  -1.55431e-15   8.88178e-16\n -1.55431e-15   2.22045e-15  -2.9976e-15\n  9.99201e-16  -2.9976e-15    4.44089e-16\n\njulia> CLSEIG\n3-element Array{Complex{Float64},1}:\n   -4.37703628399912 + 2.8107164873731247im\n   -4.37703628399912 - 2.8107164873731247im\n -1.8663764577096091 + 0.0im\n\njulia> eigvals(A-B*F)\n3-element Array{Complex{Float64},1}:\n  -4.377036283999118 - 2.8107164873731234im\n  -4.377036283999118 + 2.8107164873731234im\n -1.8663764577096063 + 0.0im\n\n\n\n\n\narec(A, B, G, R, Q, S; scaling = 'B', pow2 = false, as = false, rtol::Real = nϵ, orth = false) -> (X, EVALS, F, Z, scalinfo)\n\nComputes X, the hermitian/symmetric stabilizing solution (if as = false) or anti-stabilizing solution (if as = true) of the continuous-time algebraic Riccati equation\n\n A'X + XA - XGX - (XB+S)R^(-1)(B'X+S') + Q = 0,\n\nwhere G, R and Q are hermitian/symmetric matrices or uniform scaling operators such that R is nonsingular. Scalar-valued G, R and Q are interpreted as appropriately sized uniform scaling operators G*I, R*I and Q*I. S, if not specified, is set to S = zeros(size(B)).  For well conditioned R, the Schur method of [1] is used. For ill-conditioned R or if orth = true,  the generalized Schur method of [2] is used. \n\nTo enhance the accuracy of computations, a block oriented scaling of matrices G, Q, R and S is performed  using the default setting scaling = 'B'. This scaling is performed only if norm(Q) > max(norm(G), norm(B)^2/norm(R)). A general, eigenvalue computation oriented scaling combined with a block scaling is used if scaling = 'G' is selected.  An alternative, experimental structure preserving scaling can be performed using the option scaling = 'S'.  If orth = true, two experimental scaling procedures  can be activated using the options scaling = 'D' and scaling = 'T'. Scaling can be disabled with the choice scaling = 'N'.\n\nBy default, the lower bound for the 1-norm reciprocal condition number rtol is n*ϵ, where n is the order of A and ϵ is the machine epsilon of the element type of A.\n\nEVALS is a vector containing the (stable or anti-stable) eigenvalues of A-BF-GX.\n\nF is the stabilizing or anti-stabilizing gain matrix F = R^(-1)(B'X+S').\n\nZ = [U; V; W] is a basis for the relevant stable/anti-stable deflating subspace  such that X = Sx*(V/U)*Sxi and  F = -Sr*(W/U)*Sxi,  where Sx, Sxi and Sr are diagonal scaling matrices contained in the named tuple scalinfo  as scalinfo.Sx, scalinfo.Sxi and scalinfo.Sr, respectively. An orthogonal basis Z can be determined, with an increased computational cost, by setting orth = true.\n\nReference:\n\n[1] Laub, A.J., A Schur Method for Solving Algebraic Riccati equations.     IEEE Trans. Auto. Contr., AC-24, pp. 913-921, 1979.\n\n[2] W.F. Arnold, III and A.J. Laub,     Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,     Proc. IEEE, 72:1746-1754, 1984.\n\n\n\n\n\n","category":"function"},{"location":"riccati.html#MatrixEquations.ared","page":"Riccati Matrix Equation Solvers","title":"MatrixEquations.ared","text":"ared(A, B, R, Q, S; scaling = 'B', pow2 = false, as = false, rtol::Real = nϵ) -> (X, EVALS, F, Z, scalinfo)\n\nCompute X, the hermitian/symmetric stabilizing solution (if as = false) or anti-stabilizing solution (if as = true) of the discrete-time algebraic Riccati equation\n\nA'XA - X - (A'XB+S)(R+B'XB)^(-1)(B'XA+S') + Q = 0,\n\nwhere R and Q are hermitian/symmetric matrices. Scalar-valued R and Q are interpreted as appropriately sized uniform scaling operators R*I and Q*I. S, if not specified, is set to S = zeros(size(B)).\n\nTo enhance the accuracy of computations, a block oriented scaling of matrices R, Q and S is performed  using the default setting scaling = 'B'. This scaling is performed only if norm(Q) > norm(B)^2/norm(R). Alternative scaling can be performed using the options scaling = 'S', for a special structure preserving scaling, and  scaling = 'G', for a general eigenvalue computation oriented scaling. Experimental scaling procedures  can be activated using the options scaling = 'D', scaling = 'R' and scaling = 'T'. Scaling can be disabled with the choice scaling = 'N'. If pow2 = true, the scaling elements are enforced to the nearest power of 2 (default: pow2 = false).\n\nBy default, the lower bound for the 1-norm reciprocal condition number rtol is n*ϵ, where n is the order of A and ϵ is the machine epsilon of the element type of A.\n\nEVALS is a vector containing the (stable) eigenvalues of A-BF.\n\nF is the stabilizing gain matrix F = (R+B'XB)^(-1)(B'XA+S').\n\nZ = [U; V; W] is an orthogonal basis for the relevant stable/anti-stable deflating subspace  such that X = Sx*(V/U)*Sxi and  F = -Sr*(W/U)*Sxi,  where Sx, Sxi and Sr are diagonal scaling matrices contained in the named tuple scalinfo  as scalinfo.Sx, scalinfo.Sxi and scalinfo.Sr, respectively.\n\nReference:\n\n[1] W.F. Arnold, III and A.J. Laub,     Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,     Proc. IEEE, 72:1746-1754, 1984.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [ 0. 1.; 0. 0. ]\n2×2 Array{Float64,2}:\n 0.0  1.0\n 0.0  0.0\n\njulia> B = [ 0.; sqrt(2.) ]\n2-element Array{Float64,1}:\n 0.0\n 1.4142135623730951\n\njulia> R = 1.\n1.0\n\njulia> Q = [ 1. -1.; -1. 1. ]\n2×2 Array{Float64,2}:\n  1.0  -1.0\n -1.0   1.0\n\njulia> X, CLSEIG, F = ared(A,B,R,Q);\n\njulia> X\n2×2 Array{Float64,2}:\n  1.0  -1.0\n -1.0   1.5\n\njulia> A'*X*A-X-A'*X*B*inv(R+B'*X*B)*B'*X*A+Q\n2×2 Array{Float64,2}:\n  0.0          -3.33067e-16\n -3.33067e-16   8.88178e-16\n\njulia> CLSEIG\n2-element Array{Complex{Float64},1}:\n 0.4999999999999998 - 0.0im\n               -0.0 - 0.0im\n\njulia> eigvals(A-B*F)\n2-element Array{Float64,1}:\n -2.7755575615628914e-16\n  0.5\n\n\n\n\n\n","category":"function"},{"location":"riccati.html#Generalized-Riccati-Matrix-Equations","page":"Riccati Matrix Equation Solvers","title":"Generalized Riccati Matrix Equations","text":"","category":"section"},{"location":"riccati.html","page":"Riccati Matrix Equation Solvers","title":"Riccati Matrix Equation Solvers","text":"garec\ngared","category":"page"},{"location":"riccati.html#MatrixEquations.garec","page":"Riccati Matrix Equation Solvers","title":"MatrixEquations.garec","text":"garec(A, E, G, Q = 0; scaling = 'B', pow2 = false, as = false, rtol::Real = nϵ) -> (X, EVALS, Z, scalinfo)\n\nCompute X, the hermitian/symmetric stabilizing solution (if as = false) or anti-stabilizing solution (if as = true) of the generalized continuous-time algebraic Riccati equation\n\nA'XE + E'XA - E'XGXE + Q = 0,\n\nwhere G and Q are hermitian/symmetric matrices or uniform scaling operators and E is a nonsingular matrix. Scalar-valued G and Q are interpreted as appropriately sized uniform scaling operators G*I and Q*I. The generalized Schur method of [1] is used. \n\nTo enhance the accuracy of computations, a block scaling of matrices G and Q is performed, if   the default setting scaling = 'B' is used. This scaling is however performed only if norm(Q) > norm(G). A general, eigenvalue computation oriented scaling combined with a block scaling is used if scaling = 'G' is selected.  An alternative, experimental structure preserving scaling can be performed using the option scaling = 'S'.  Scaling can be disabled with the choice scaling = 'N'. If pow2 = true, the scaling elements are enforced to the nearest power of 2 (default: pow2 = false).\n\nBy default, the lower bound for the 1-norm reciprocal condition number rtol is n*ϵ, where n is the order of A and ϵ is the machine epsilon of the element type of A.\n\nEVALS is a vector containing the (stable or anti-stable) generalized eigenvalues of the pair (A-GXE,E).\n\nZ = [U; V] is an orthogonal basis for the stable/anti-stable deflating subspace such that X = Sx*(V/U)*Sxi/E,  where Sx and Sxi are diagonal scaling matrices contained in the named tuple scalinfo  as scalinfo.Sx and scalinfo.Sxi, respectively.\n\nNote: To solve the continuous-time algebraic Riccati equation\n\n A'XE + E'XA - E'XBR^(-1)B'XE + Q = 0,\n\nwith R a hermitian/symmetric matrix and B a compatible size matrix, G = BR^(-1)B' must be provided.  This approach is not numerically suited when R is ill-conditioned and/or B has large norm.  \n\nReference:\n\n[1] W.F. Arnold, III and A.J. Laub,     Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,     Proc. IEEE, 72:1746-1754, 1984.\n\n\n\n\n\ngarec(A, E, B, R, Q, S; scaling = 'B', pow2 = false, as = false, rtol::Real = nϵ) -> (X, EVALS, F, Z, scalinfo)\n\nCompute X, the hermitian/symmetric stabilizing solution (if as = false) or anti-stabilizing solution (if as = true) of the generalized continuous-time algebraic Riccati equation\n\nA'XE + E'XA - (E'XB+S)R^(-1)(B'XE+S') + Q = 0,\n\nwhere R and Q are hermitian/symmetric matrices such that R is nonsingular, and E is a nonsingular matrix. Scalar-valued R and Q are interpreted as appropriately sized uniform scaling operators R*I and Q*I. S, if not specified, is set to S = zeros(size(B)).  The generalized Schur method of [1] is used. \n\nTo enhance the accuracy of computations, a block oriented scaling of matrices R, Q and S is performed  using the default setting scaling = 'B'. This scaling is performed only if norm(Q) > norm(B)^2/norm(R). Alternative scaling can be performed using the options scaling = 'S', for a special structure preserving scaling, and  scaling = 'G', for a general eigenvalue computation oriented scaling. Two experimental scaling procedures  can be activated using the options scaling = 'D' and scaling = 'T'. Scaling can be disabled with the choice scaling = 'N'. If pow2 = true, the scaling elements are enforced to the nearest power of 2 (default: pow2 = false).\n\nBy default, the lower bound for the 1-norm reciprocal condition number rtol is n*ϵ, where n is the order of A and ϵ is the machine epsilon of the element type of A.\n\nEVALS is a vector containing the (stable or anti-stable) generalized eigenvalues of the pair (A-BF,E).\n\nF is the stabilizing/anti-stabilizing gain matrix F = R^(-1)(B'XE+S').\n\nZ = [U; V; W] is an orthogonal basis for the relevant stable/anti-stable deflating subspace  such that X = Sx*(V/U)*Sxi and  F = -Sr*(W/U)*Sxi,  where Sx, Sxi and Sr are diagonal scaling matrices contained in the named tuple scalinfo  as scalinfo.Sx, scalinfo.Sxi and scalinfo.Sr, respectively.\n\nReference:\n\n[1] W.F. Arnold, III and A.J. Laub,     Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,     Proc. IEEE, 72:1746-1754, 1984.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [-6. -2. 1.; 5. 1. -1; -4. -2. -1.]\n3×3 Array{Float64,2}:\n -6.0  -2.0   1.0\n  5.0   1.0  -1.0\n -4.0  -2.0  -1.0\n\njulia> E = [10. 3. 0.; 0. 5. -1.; 0. 0. 10.]\n3×3 Array{Float64,2}:\n 10.0  3.0   0.0\n  0.0  5.0  -1.0\n  0.0  0.0  10.0\n\njulia> B = [1. 2.; 2. 0.; 0. 1.]\n3×2 Array{Float64,2}:\n 1.0  2.0\n 2.0  0.0\n 0.0  1.0\n\njulia> R = [1. 0.; 0. 5.]\n2×2 Array{Float64,2}:\n 1.0  0.0\n 0.0  5.0\n\njulia> X, CLSEIG, F = garec(A,E,B,R,2I);\n\njulia> X\n3×3 Array{Float64,2}:\n  0.0502214   0.0284089   -0.0303703\n  0.0284089   0.111219    -0.00259162\n -0.0303703  -0.00259162   0.0618395\n\njulia> A'*X*E+E'*X*A-E'*X*B*inv(R)*B'*X*E+2I\n3×3 Array{Float64,2}:\n  1.55431e-15  -1.9984e-15   -3.33067e-15\n -1.77636e-15   1.33227e-15  -3.33067e-15\n -2.88658e-15  -3.21965e-15   1.11022e-15\n\njulia> CLSEIG\n3-element Array{Complex{Float64},1}:\n  -0.6184265391601464 + 0.2913286844595737im\n  -0.6184265391601464 - 0.2913286844595737im\n -0.21613059964451786 + 0.0im\n\njulia> eigvals(A-B*F,E)\n3-element Array{Complex{Float64},1}:\n -0.6184265391601462 - 0.29132868445957383im\n -0.6184265391601462 + 0.2913286844595739im\n  -0.216130599644518 + 0.0im\n\n\n\n\n\ngarec(A, E, B, G, R, Q, S; scaling = 'B', pw2 = false, as = false, rtol::Real = nϵ) -> (X, EVALS, F, Z, scalinfo)\n\nCompute X, the hermitian/symmetric stabilizing solution (if as = false) or anti-stabilizing solution (if as = true) of the generalized continuous-time algebraic Riccati equation\n\nA'XE + E'XA - E'XGXE - (E'XB+S)R^(-1)(B'XE+S') + Q = 0,\n\nwhere G, Q and R are hermitian/symmetric matrices such that R is nonsingular, and E is a nonsingular matrix. Scalar-valued G, R and Q are interpreted as appropriately sized uniform scaling operators G*I, R*I and Q*I.\n\nThe generalized Schur method of [1] is used.  To enhance the accuracy of computations, a block oriented scaling of matrices G, R, Q and S is performed  using the default setting scaling = 'B'. This scaling is performed only if norm(Q) > max(norm(G), norm(B)^2/norm(R)). Alternative scaling can be performed using the options scaling = 'S', for a special structure preserving scaling, and  scaling = 'G', for a general eigenvalue computation oriented scaling. Two experimental scaling procedures  can be activated using the options scaling = 'D' and scaling = 'T'. Scaling can be disabled with the choice scaling = 'N'. If pow2 = true, the scaling elements are enforced to the nearest power of 2 (default: pow2 = false).\n\nBy default, the lower bound for the 1-norm reciprocal condition number rtol is n*ϵ, where n is the order of A and ϵ is the machine epsilon of the element type of A.\n\nEVALS is a vector containing the (stable or anti-stable) generalized eigenvalues of the pair (A-BF-GXE,E).\n\nF is the stabilizing/anti-stabilizing gain matrix F = R^(-1)(B'XE+S').\n\nZ = [U; V; W] is an orthogonal basis for the relevant stable/anti-stable deflating subspace  such that X = Sx*(V/U)*Sxi and  F = -Sr*(W/U)*Sxi,  where Sx, Sxi and Sr are diagonal scaling matrices contained in the named tuple scalinfo  as scalinfo.Sx, scalinfo.Sxi and scalinfo.Sr, respectively.\n\nReference:\n\n[1] W.F. Arnold, III and A.J. Laub,     Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,     Proc. IEEE, 72:1746-1754, 1984.\n\n\n\n\n\n","category":"function"},{"location":"riccati.html#MatrixEquations.gared","page":"Riccati Matrix Equation Solvers","title":"MatrixEquations.gared","text":"gared(A, E, B, R, Q, S; scaling = 'B', pow2 = false, as = false, rtol::Real = nϵ) -> (X, EVALS, F, Z, scalinfo)\n\nCompute X, the hermitian/symmetric stabilizing solution (if as = false) or anti-stabilizing solution (if as = true) of the generalized discrete-time algebraic Riccati equation\n\nA'XA - E'XE - (A'XB+S)(R+B'XB)^(-1)(B'XA+S') + Q = 0,\n\nwhere R and Q are hermitian/symmetric matrices, and E ist non-singular. Scalar-valued R and Q are interpreted as appropriately sized uniform scaling operators R*I and Q*I. S, if not specified, is set to S = zeros(size(B)).\n\nTo enhance the accuracy of computations, a block oriented scaling of matrices R, Q and S is performed  using the default setting scaling = 'B'. This scaling is performed only if norm(Q) > norm(B)^2/norm(R). Alternative scaling can be performed using the options scaling = 'S', for a special structure preserving scaling, and  scaling = 'G', for a general eigenvalue computation oriented scaling. Experimental scaling procedures  can be activated using the options scaling = 'D', scaling = 'R' and scaling = 'T'. Scaling can be disabled with the choice scaling = 'N'. If pow2 = true, the scaling elements are enforced to the nearest power of 2 (default: pow2 = false).\n\nBy default, the lower bound for the 1-norm reciprocal condition number rtol is n*ϵ, where n is the order of A and ϵ is the machine epsilon of the element type of A.\n\nEVALS is a vector containing the (stable or anti-stable) generalized eigenvalues of the pair (A-BF,E).\n\nF is the stabilizing/anti-stabilizing gain matrix F = (R+B'XB)^(-1)(B'XA+S').\n\nZ = [U; V; W] is an orthogonal basis for the relevant stable/anti-stable deflating subspace  such that X = Sx*(V/U)*Sxi and  F = -Sr*(W/U)*Sxi,  where Sx, Sxi and Sr are diagonal scaling matrices contained in the named tuple scalinfo  as scalinfo.Sx, scalinfo.Sxi and scalinfo.Sr, respectively.\n\nReference:\n\n[1] W.F. Arnold, III and A.J. Laub,     Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,     Proc. IEEE, 72:1746-1754, 1984.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [-6. -2. 1.; 5. 1. -1; -4. -2. -1.]\n3×3 Array{Float64,2}:\n -6.0  -2.0   1.0\n  5.0   1.0  -1.0\n -4.0  -2.0  -1.0\n\njulia> E = [10. 3. 0.; 0. 5. -1.; 0. 0. 10.]\n3×3 Array{Float64,2}:\n 10.0  3.0   0.0\n  0.0  5.0  -1.0\n  0.0  0.0  10.0\n\njulia> B = [1. 2.; 2. 0.; 0. 1.]\n3×2 Array{Float64,2}:\n 1.0  2.0\n 2.0  0.0\n 0.0  1.0\n\njulia> R = [1. 0.; 0. 5.]\n2×2 Array{Float64,2}:\n 1.0  0.0\n 0.0  5.0\n\njulia> X, CLSEIG, F = gared(A,E,B,R,2I);\n\njulia> X\n3×3 Array{Float64,2}:\n  0.065865   -0.0147205  -0.0100407\n -0.0147205   0.0885939   0.0101422\n -0.0100407   0.0101422   0.0234425\n\njulia> A'*X*A-E'*X*E-A'*X*B*inv(R+B'*X*B)*B'*X*A+2I\n3×3 Array{Float64,2}:\n -1.33227e-15  -2.48412e-15   1.38778e-16\n -2.498e-15    -4.44089e-16  -6.50521e-16\n  1.80411e-16  -5.91541e-16  -1.33227e-15\n\njulia> CLSEIG\n3-element Array{Complex{Float64},1}:\n  -0.084235615751339 - 0.0im\n  -0.190533552034239 - 0.0im\n -0.5238922629921539 - 0.0im\n\njulia> eigvals(A-B*F,E)\n3-element Array{Float64,1}:\n -0.5238922629921539\n -0.19053355203423886\n -0.08423561575133902\n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#Linear-Operators-Related-to-Matrix-Equation-Solvers","page":"Linear Operators Related to Matrix Equation Solvers","title":"Linear Operators Related to Matrix Equation Solvers","text":"","category":"section"},{"location":"meoperators.html#Lyapunov-and-Lyapunov-like-Operators","page":"Linear Operators Related to Matrix Equation Solvers","title":"Lyapunov and Lyapunov-like Operators","text":"","category":"section"},{"location":"meoperators.html","page":"Linear Operators Related to Matrix Equation Solvers","title":"Linear Operators Related to Matrix Equation Solvers","text":"lyapop\ninvlyapop\nlyaplikeop\ntulyaplikeop\nhulyaplikeop","category":"page"},{"location":"meoperators.html#MatrixEquations.lyapop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.lyapop","text":"L = lyapop(A; disc = false, her = false)\n\nDefine, for an n x n matrix A, the continuous Lyapunov operator L:X -> AX+XA' if disc = false or the discrete Lyapunov operator L:X -> AXA'-X if disc = true. If her = false the Lyapunov operator L:X -> Y maps general square matrices X into general square matrices Y, and the associated matrix M = Matrix(L) is n^2 times n^2. If her = true the Lyapunov operator L:X -> Y maps symmetric/Hermitian matrices X into symmetric/Hermitian matrices Y, and the associated matrix M = Matrix(L) is n(n+1)2 times n(n+1)2. For the definitions of the Lyapunov operators see:\n\nM. Konstantinov, V. Mehrmann, P. Petkov. On properties of Sylvester and Lyapunov operators. Linear Algebra and its Applications 312:35–71, 2000.\n\n\n\n\n\nL = lyapop(A, E; disc = false, her = false)\n\nDefine, for a pair (A,E) of n x n matrices, the continuous Lyapunov operator L:X -> AXE'+EXA' if disc = false or the discrete Lyapunov operator L:X -> AXA'-EXE' if disc = true. If her = false the Lyapunov operator L:X -> Y maps general square matrices X into general square matrices Y, and the associated matrix M = Matrix(L) is n^2 times n^2. If her = true the Lyapunov operator L:X -> Y maps symmetric/Hermitian matrices X into symmetric/Hermitian matrices Y, and the associated M = Matrix(L) is a n(n+1)2 times n(n+1)2. For the definitions of the Lyapunov operators see:\n\nM. Konstantinov, V. Mehrmann, P. Petkov. On properties of Sylvester and Lyapunov operators. Linear Algebra and its Applications 312:35–71, 2000.\n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#MatrixEquations.invlyapop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.invlyapop","text":"LINV = invlyapop(A; disc = false, her = false)\n\nDefine LINV, the inverse of the continuous Lyapunov operator L:X -> AX+XA' for disc = false or the inverse of the discrete Lyapunov operator L:X -> AXA'-X for disc = true, where A is an n x n matrix. If her = false the inverse Lyapunov operator LINV:Y -> X maps general square matrices Y into general square matrices X, and the associated matrix M = Matrix(LINV) is n^2 times n^2. If her = true the inverse Lyapunov operator LINV:Y -> X maps symmetric/Hermitian matrices Y into symmetric/Hermitian matrices X, and the associated matrix M = Matrix(LINV) is n(n+1)2 times n(n+1)2. For the definitions of the Lyapunov operators see:\n\nM. Konstantinov, V. Mehrmann, P. Petkov. On properties of Sylvester and Lyapunov operators. Linear Algebra and its Applications 312:35–71, 2000.\n\n\n\n\n\nLINV = invlyapop(A, E; disc = false, her = false)\n\nDefine LINV, the inverse of the continuous Lyapunov operator L:X -> AXE'+EXA' for disc = false or the inverse of the discrete Lyapunov operator L:X -> AXA'-EXE' for disc = true, where (A,E) is a pair of n x n matrices. If her = false the inverse Lyapunov operator LINV:Y -> X maps general square matrices Y into general square matrices X, and the associated matrix M = Matrix(LINV) is n^2 times n^2. If her = true the inverse Lyapunov operator LINV:Y -> X maps symmetric/Hermitian matrices Y into symmetric/Hermitian matrices X, and the associated matrix M = Matrix(LINV) is n(n+1)2 times n(n+1)2. For the definitions of the Lyapunov operators see:\n\nM. Konstantinov, V. Mehrmann, P. Petkov. On properties of Sylvester and Lyapunov operators. Linear Algebra and its Applications 312:35–71, 2000.\n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#MatrixEquations.lyaplikeop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.lyaplikeop","text":"L = lyaplikeop(A; isig = 1, adj = false, htype = false)\n\nFor a matrix A, define for adj = false the continuous T-Lyapunov operator L:X -> A*X+isig*transpose(X)*transpose(A) if htype = false or the continuous H-Lyapunov operator L:X -> A*X+isig*X'*A' if htype = true, or  define for adj = true the continuous T-Lyapunov operator L:X -> A*transpose(X)+X*transpose(A) if htype = false, or the continuous H-Lyapunov operator  L:X -> A*X'+isig*X*A' if htype = true. \n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#MatrixEquations.tulyaplikeop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.tulyaplikeop","text":"L = tulyaplikeop(U; adj = false)\n\nDefine, for an upper triangular matrix U, the continuous T-Lyapunov operator L:X -> transpose(U)*X+transpose(X)*U, if adj = false, or L:X -> U*transpose(X)+X*transpose(U) if adj = true.\n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#MatrixEquations.hulyaplikeop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.hulyaplikeop","text":"L = hulyaplikeop(U; adj = false)\n\nDefine, for an upper triangular matrix U, the continuous H-Lyapunov operator L:X -> U'*X+X'*U, if adj = false, L:X -> U*X'+X*U' if adj = true. \n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#Sylvester-and-Sylvester-like-Operators","page":"Linear Operators Related to Matrix Equation Solvers","title":"Sylvester and Sylvester-like Operators","text":"","category":"section"},{"location":"meoperators.html","page":"Linear Operators Related to Matrix Equation Solvers","title":"Linear Operators Related to Matrix Equation Solvers","text":"sylvop\ninvsylvop\ngsylvop","category":"page"},{"location":"meoperators.html#MatrixEquations.sylvop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.sylvop","text":"M = sylvop(A, B; disc = false)\n\nDefine the continuous Sylvester operator M: X -> AX+XB if disc = false or the discrete Sylvester operator M: X -> AXB+X if disc = true, where A and B are square matrices.\n\n\n\n\n\nM = sylvop(A, B, C, D)\n\nDefine the generalized Sylvester operator M: X -> AXB+CXD, where (A,C) and (B,D) are pairs of square matrices.\n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#MatrixEquations.invsylvop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.invsylvop","text":"MINV = invsylvop(A, B; disc = false)\n\nDefine MINV, the inverse of the continuous Sylvester operator  M: X -> AX+XB if disc = false or of the discrete Sylvester operator M: X -> AXB+X if disc = true, where A and B are square matrices.\n\n\n\n\n\nMINV = invsylvop(A, B, C, D)\n\nDefine MINV, the inverse of the generalized Sylvester operator M: X -> AXB+CXD, where (A,C) and (B,D) are pairs of square matrices.\n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#MatrixEquations.gsylvop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.gsylvop","text":"M = gsylvop(A, B, C, D; mx, nx, htype = false)\n\nDefine the generalized T-Sylvester operator M: X -> ∑ A_i*X*B_i + ∑ C_j'*transpose(X)*D_j, if htype = false or the generalized H-Sylvester operator M: X -> ∑ A_i*X*B_i + ∑ C_j'*X'*D_j, if htype = true. A_i and C_j are matrices having the same row dimension and B_i and D_j are matrices having the same column dimension.  A_i and B_i are contained in the k-vectors of matrices A and B, respectively, and  C_j and D_j are contained in the l-vectors of matrices C and D, respectively. Any of the component matrices can be given as an UniformScaling.  The keyword parameters mx and nx can be used to specify the row and column dimensions of X, if they cannot be inferred from the data.   \n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#Sylvester-System-Operators","page":"Linear Operators Related to Matrix Equation Solvers","title":"Sylvester System Operators","text":"","category":"section"},{"location":"meoperators.html","page":"Linear Operators Related to Matrix Equation Solvers","title":"Linear Operators Related to Matrix Equation Solvers","text":"sylvsysop\ninvsylvsysop","category":"page"},{"location":"meoperators.html#MatrixEquations.sylvsysop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.sylvsysop","text":"M = sylvsysop(A, B, C, D)\n\nDefine the operator M: (X,Y) -> (AX+YB, CX+YD), where (A,C) and (B,D) are pairs of square matrices.\n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#MatrixEquations.invsylvsysop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.invsylvsysop","text":"MINV = invsylvsysop(A, B, C, D)\n\nDefine MINV, the inverse of the linear operator M: (X,Y) -> (AX+YB, CX+YD ), where (A,C) and (B,D) a pairs of square matrices.\n\n\n\n\n\n","category":"function"},{"location":"meoperators.html#Elementary-Matrix-Operators","page":"Linear Operators Related to Matrix Equation Solvers","title":"Elementary Matrix Operators","text":"","category":"section"},{"location":"meoperators.html","page":"Linear Operators Related to Matrix Equation Solvers","title":"Linear Operators Related to Matrix Equation Solvers","text":"trmatop\neliminationop\nduplicationop","category":"page"},{"location":"meoperators.html#MatrixEquations.trmatop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.trmatop","text":"M = trmatop(n, m)\nM = trmatop(A)\n\nDefine the transposition operator M: X -> X' for all n x m matrices or for all matrices of size of A.  The corresponding commutation matrix (see here)  can be generated as Matrix(M).\n\n\n\n\n\n","category":"type"},{"location":"meoperators.html#MatrixEquations.eliminationop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.eliminationop","text":"M = eliminationop(n)\nM = eliminationop(A)\n\nDefine the elimination operator of all n×n matrices to select their upper triangular parts or of all square matrices of size of A.   See here for the  definition of an elimination matrix for the selection of lower triangular parts.\n\n\n\n\n\n","category":"type"},{"location":"meoperators.html#MatrixEquations.duplicationop","page":"Linear Operators Related to Matrix Equation Solvers","title":"MatrixEquations.duplicationop","text":"M = duplicationop(n)\nM = duplicationop(A)\n\nDefine the duplication operator of all n×n matrices to reconstruct a hermitian matrix from its upper triangular elements  or of all square matrices of size of A. See here for the  definition of a duplication matrix from the lower triangular parts.\n\n\n\n\n\n","category":"type"},{"location":"sylvester.html#Sylvester-Matrix-Equation-Solvers","page":"Sylvester Matrix Equation Solvers","title":"Sylvester Matrix Equation Solvers","text":"","category":"section"},{"location":"sylvester.html#Standard-Sylvester-Matrix-Equations","page":"Sylvester Matrix Equation Solvers","title":"Standard Sylvester Matrix Equations","text":"","category":"section"},{"location":"sylvester.html","page":"Sylvester Matrix Equation Solvers","title":"Sylvester Matrix Equation Solvers","text":"sylvc\nsylvcs!\nsylvd\nsylvds!","category":"page"},{"location":"sylvester.html#MatrixEquations.sylvc","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.sylvc","text":"X = sylvc(A,B,C)\n\nSolve the continuous Sylvester matrix equation\n\n            AX + XB = C\n\nusing the Bartels-Stewart Schur form based approach. A and B are square matrices, and A and -B must not have common eigenvalues.\n\nThe following particular cases are also adressed:\n\nX = sylvc(α*I,B,C)  or  X = sylvc(α,B,C)\n\nSolve the matrix equation X(αI+B)  = C.\n\nX = sylvc(A,β*I,C)  or  X = sylvc(A,β,C)\n\nSolve the matrix equation (A+βI)X = C.\n\nX = sylvc(α*I,β*I,C)  or  sylvc(α,β,C)\n\nSolve the matrix equation (α+β)X = C.\n\nx = sylvc(α,β,γ)\n\nSolve the equation (α+β)x = γ.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [-1. -2.; 2. -1.]\n2×2 Array{Float64,2}:\n -1.0  -2.0\n  2.0  -1.0\n\njulia> X = sylvc(A, B, C)\n2×2 Array{Float64,2}:\n -4.46667   1.93333\n  3.73333  -1.8\n\njulia> A*X + X*B - C\n2×2 Array{Float64,2}:\n  2.66454e-15  1.77636e-15\n -3.77476e-15  4.44089e-16\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#MatrixEquations.sylvcs!","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.sylvcs!","text":"sylvcs!(A,B,C; adjA = false, adjB = false)\n\nSolve the continuous Sylvester matrix equation\n\n            op(A)X + Xop(B) =  C,\n\nwhere op(A) = A or op(A) = A' if adjA = false or adjA = true, respectively, and op(B) = B or op(B) = B' if adjB = false or adjB = true, respectively. A and B are square matrices in Schur forms, and A and -B must not have common eigenvalues. C contains on output the solution X.\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#MatrixEquations.sylvd","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.sylvd","text":"X = sylvd(A,B,C)\n\nSolve the discrete Sylvester matrix equation\n\n            AXB + X = C\n\nusing an extension of the Bartels-Stewart Schur form based approach. A and B are square matrices, and A and -B must not have common reciprocal eigenvalues.\n\nThe following particular cases are also adressed:\n\nX = sylvd(α*I,B,C)  or  X = sylvd(α,B,C)\n\nSolve the matrix equation X(αB+I)  = C.\n\nX = sylvd(A,β*I,C)   or  X = sylvd(A,β,C)\n\nSolve the matrix equation (βA+I)X = C.\n\nX = sylvd(α*I,β*I,C)  or  X = sylvd(α,β,C)\n\nSolve the matrix equation (αβ+1)X = C.\n\nx = sylvd(α,β,γ)\n\nSolve the equation (αβ+1)x = γ.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [-1. -2.; 2. -1.]\n2×2 Array{Float64,2}:\n -1.0  -2.0\n  2.0  -1.0\n\njulia> X = sylvd(A, B, C)\n2×2 Array{Float64,2}:\n -2.46667  -2.73333\n  2.4       1.86667\n\njulia> A*X*B + X - C\n2×2 Array{Float64,2}:\n  8.88178e-16   8.88178e-16\n -3.9968e-15   -5.55112e-15\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#MatrixEquations.sylvds!","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.sylvds!","text":"sylvds!(A,B,C; adjA = false, adjB = false)\n\nSolve the discrete Sylvester matrix equation\n\n            op(A)Xop(B) + X =  C,\n\nwhere op(A) = A or op(A) = A' if adjA = false or adjA = true, respectively, and op(B) = B or op(B) = B' if adjB = false or adjB = true, respectively. A and B are square matrices in Schur forms, and A and -B must not have common reciprocal eigenvalues. C contains on output the solution X.\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#Generalized-Sylvester-Matrix-Equations","page":"Sylvester Matrix Equation Solvers","title":"Generalized Sylvester Matrix Equations","text":"","category":"section"},{"location":"sylvester.html","page":"Sylvester Matrix Equation Solvers","title":"Sylvester Matrix Equation Solvers","text":"gsylv\ngsylvs!","category":"page"},{"location":"sylvester.html#MatrixEquations.gsylv","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.gsylv","text":"X = gsylv(A,B,C,D,E)\n\nSolve the generalized Sylvester matrix equation\n\n          AXB + CXD = E\n\nusing a generalized Schur form based approach. A, B, C and D are square matrices. The pencils A-λC and D+λB must be regular and must not have common eigenvalues.\n\nThe following particular cases are also adressed:\n\nX = gsylv(A,B,E)\n\nSolve the generalized Sylvester matrix equation AXB  = E.\n\nX = gsylv(A,B,γ*I,E)  or  X = gsylv(A,B,γ,E)\n\nSolve the generalized Sylvester matrix equation AXB +γX = E.\n\nX = gsylv(A,B,γ*I,D,E)  or  X = gsylv(A,B,γ,D,E)\n\nSolve the generalized Sylvester matrix equation AXB +γXD = E.\n\nX = gsylv(A,B,C,δ*I,E)  or  X = gsylv(A,B,C,δ,E)\n\nSolve the generalized Sylvester matrix equation AXB +CXδ = E.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [-1. -2.; 2. -1.]\n2×2 Array{Float64,2}:\n -1.0  -2.0\n  2.0  -1.0\n\njulia> D = [1. -2.; -2. -1.]\n2×2 Array{Float64,2}:\n  1.0  -2.0\n -2.0  -1.0\n\njulia> E = [1. -1.; -2. 2.]\n2×2 Array{Float64,2}:\n  1.0  -1.0\n -2.0   2.0\n\njulia> X = gsylv(A, B, C, D, E)\n2×2 Array{Float64,2}:\n -0.52094   -0.0275792\n -0.168539   0.314607\n\njulia> A*X*B + C*X*D - E\n2×2 Array{Float64,2}:\n 4.44089e-16  8.88178e-16\n 6.66134e-16  0.0\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#MatrixEquations.gsylvs!","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.gsylvs!","text":"X = gsylvs!(A,B,C,D,E; adjAC=false, adjBD=false, CASchur = false, DBSchur = false)\n\nSolve the generalized Sylvester matrix equation\n\n            op1(A)Xop2(B) + op1(C)Xop2(D) = E,\n\nwhere A, B, C and D are square matrices, and\n\nop1(A) = A and op1(C) = C if adjAC = false;\n\nop1(A) = A' and op1(C) = C' if adjAC = true;\n\nop2(B) = B and op2(D) = D if adjBD = false;\n\nop2(B) = B' and op2(D) = D' if adjBD = true.\n\nThe matrix pair (A,C) is in a generalized real or complex Schur form. The matrix pair (B,D) is in a generalized real or complex Schur form if DBSchur = false or the matrix pair (D,B) is in a generalized real or complex Schur form if DBSchur = true. The pencils A-λC and D+λB must be regular and must not have common eigenvalues.\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#Sylvester-Systems-of-Matrix-Equation","page":"Sylvester Matrix Equation Solvers","title":"Sylvester Systems of Matrix Equation","text":"","category":"section"},{"location":"sylvester.html","page":"Sylvester Matrix Equation Solvers","title":"Sylvester Matrix Equation Solvers","text":"sylvsys\nsylvsyss!\ndsylvsys\ndsylvsyss!","category":"page"},{"location":"sylvester.html#MatrixEquations.sylvsys","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.sylvsys","text":"(X,Y) = sylvsys(A,B,C,D,E,F)\n\nSolve the Sylvester system of matrix equations\n\n            AX + YB = C\n            DX + YE = F,\n\nwhere (A,D), (B,E) are pairs of square matrices of the same size. The pencils A-λD and -B+λE must be regular and must not have common eigenvalues.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [-1. -2.; 2. -1.]\n2×2 Array{Float64,2}:\n -1.0  -2.0\n  2.0  -1.0\n\njulia> D = [1. -2.; -2. -1.]\n2×2 Array{Float64,2}:\n  1.0  -2.0\n -2.0  -1.0\n\njulia> E = [1. -1.; -2. 2.]\n2×2 Array{Float64,2}:\n  1.0  -1.0\n -2.0   2.0\n\njulia> F = [1. -1.; -2. 2.]\n2×2 Array{Float64,2}:\n  1.0  -1.0\n -2.0   2.0\n\njulia> X, Y = sylvsys(A, B, C, D, E, F);\n\njulia> X\n2×2 Array{Float64,2}:\n  1.388  -1.388\n -0.892   0.892\n\njulia> Y\n2×2 Array{Float64,2}:\n -1.788  0.192\n  0.236  0.176\n\njulia> A*X + Y*B - C\n2×2 Array{Float64,2}:\n  6.66134e-16  2.22045e-15\n -3.10862e-15  2.66454e-15\n\njulia> D*X + Y*E - F\n2×2 Array{Float64,2}:\n  1.33227e-15  -2.22045e-15\n -4.44089e-16   4.44089e-16\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#MatrixEquations.sylvsyss!","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.sylvsyss!","text":"(X,Y) = sylvsyss!(A,B,C,D,E,F)\n\nSolve the Sylvester system of matrix equations\n\n            AX + YB = C\n            DX + YE = F,\n\nwhere (A,D), (B,E) are pairs of square matrices of the same size in generalized Schur forms. The pencils A-λD and -B+λE must be regular and must not have common eigenvalues. The computed solution (X,Y) is contained in (C,F).\n\nNote: This is an enhanced interface to the LAPACK.tgsyl! function to also cover the case when A, B, D and E are real matrices and C and F are complex matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#MatrixEquations.dsylvsys","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.dsylvsys","text":"(X,Y) = dsylvsys(A,B,C,D,E,F)\n\nSolve the dual Sylvester system of matrix equations\n\n   AX + DY = C\n   XB + YE = F ,\n\nwhere (A,D), (B,E) are pairs of square matrices of the same size. The pencils A-λD and -B+λE must be regular and must not have common eigenvalues.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [-1. -2.; 2. -1.]\n2×2 Array{Float64,2}:\n -1.0  -2.0\n  2.0  -1.0\n\njulia> D = [1. -2.; -2. -1.]\n2×2 Array{Float64,2}:\n  1.0  -2.0\n -2.0  -1.0\n\njulia> E = [1. -1.; -2. 2.]\n2×2 Array{Float64,2}:\n  1.0  -1.0\n -2.0   2.0\n\njulia> F = [1. -1.; -2. 2.]\n2×2 Array{Float64,2}:\n  1.0  -1.0\n -2.0   2.0\n\njulia> X, Y = dsylvsys(A, B, C, D, E, F);\n\njulia> X\n2×2 Array{Float64,2}:\n  2.472  -1.648\n -1.848   1.232\n\njulia> Y\n2×2 Array{Float64,2}:\n -0.496  -0.336\n  0.264   0.824\n\njulia> A*X + D*Y - C\n2×2 Array{Float64,2}:\n  4.44089e-16  0.0\n -3.55271e-15  1.55431e-15\n\njulia> X*B + Y*E - F\n2×2 Array{Float64,2}:\n -8.88178e-16   0.0\n  8.88178e-16  -4.44089e-16\n\n\n\n\n\n","category":"function"},{"location":"sylvester.html#MatrixEquations.dsylvsyss!","page":"Sylvester Matrix Equation Solvers","title":"MatrixEquations.dsylvsyss!","text":"(X,Y) = dsylvsyss!(A,B,C,D,E,F)\n\nSolve the dual Sylvester system of matrix equations\n\nA'X + D'Y = C\nXB' + YE' = F,\n\nwhere (A,D), (B,E) are pairs of square matrices of the same size in generalized Schur forms. The pencils A-λD and -B+λE must be regular and must not have common eigenvalues. The computed solution (X,Y) is contained in (C,F).\n\n\n\n\n\n","category":"function"},{"location":"lyapunov.html#Lyapunov-Matrix-Equation-Solvers","page":"Lyapunov Matrix Equation Solvers","title":"Lyapunov Matrix Equation Solvers","text":"","category":"section"},{"location":"lyapunov.html#Continuous-time-Lyapunov-Matrix-Equations","page":"Lyapunov Matrix Equation Solvers","title":"Continuous-time Lyapunov Matrix Equations","text":"","category":"section"},{"location":"lyapunov.html","page":"Lyapunov Matrix Equation Solvers","title":"Lyapunov Matrix Equation Solvers","text":"lyapc\nlyapcs!\ntlyapc\nhlyapc\ntulyapc!\nhulyapc!","category":"page"},{"location":"lyapunov.html#MatrixEquations.lyapc","page":"Lyapunov Matrix Equation Solvers","title":"MatrixEquations.lyapc","text":"X = lyapc(A, C)\n\nCompute X, the solution of the continuous Lyapunov equation\n\n  AX + XA' + C = 0,\n\nwhere A is a square real or complex matrix and C is a square matrix. A must not have two eigenvalues α and β such that α+β = 0. The solution X is symmetric or hermitian if C is a symmetric or hermitian.\n\nThe following particular cases are also adressed:\n\nX = lyapc(α*I,C)  or  X = lyapc(α,C)\n\nSolve the matrix equation (α+α')X + C = 0.\n\nx = lyapc(α,γ)\n\nSolve the equation (α+α')x + γ = 0.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> C = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyapc(A, C)\n2×2 Array{Float64,2}:\n  0.5  -0.5\n -0.5   0.25\n\njulia> A*X + X*A' + C\n2×2 Array{Float64,2}:\n -8.88178e-16   2.22045e-16\n  2.22045e-16  -4.44089e-16\n\n\n\n\n\nX = lyapc(A, E, C)\n\nCompute X, the solution of the generalized continuous Lyapunov equation\n\n AXE' + EXA' + C = 0,\n\nwhere A and E are square real or complex matrices and C is a square matrix. The pencil A-λE must not have two eigenvalues α and β such that α+β = 0. The solution X is symmetric or hermitian if C is a symmetric or hermitian.\n\nThe following particular cases are also adressed:\n\nX = lyapc(A,β*I,C)  or  X = lyapc(A,β,C)\n\nSolve the matrix equation AXβ' + βXA' + C = 0.\n\nX = lyapc(α*I,E,C)  or  X = lyapc(α,E,C)\n\nSolve the matrix equation αXE' + EXα' + C = 0.\n\nX = lyapc(α*I,β*I,C)  or  X = lyapc(α,β,C)\n\nSolve the matrix equation (αβ'+α'β)X + C = 0.\n\nx = lyapc(α,β,γ)\n\nSolve the equation (αβ'+α'β)x + γ = 0.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> E = [ 1. 2.; 0. 1.]\n2×2 Array{Float64,2}:\n 1.0  2.0\n 0.0  1.0\n\njulia> C = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyapc(A, E, C)\n2×2 Array{Float64,2}:\n -2.5   2.5\n  2.5  -2.25\n\njulia> A*X*E' + E*X*A' + C\n2×2 Array{Float64,2}:\n -5.32907e-15  -2.66454e-15\n -4.44089e-15   0.0\n\n\n\n\n\n","category":"function"},{"location":"lyapunov.html#MatrixEquations.lyapcs!","page":"Lyapunov Matrix Equation Solvers","title":"MatrixEquations.lyapcs!","text":"lyapcs!(A,C;adj = false)\n\nSolve the continuous Lyapunov matrix equation\n\n            op(A)X + Xop(A)' + C = 0,\n\nwhere op(A) = A if adj = false and op(A) = A' if adj = true. A is a square real matrix in a real Schur form, or a square complex matrix in a complex Schur form and C is a symmetric or hermitian matrix. A must not have two eigenvalues α and β such that α+β = 0. C contains on output the solution X.\n\n\n\n\n\nlyapcs!(A, E, C; adj = false)\n\nSolve the generalized continuous Lyapunov matrix equation\n\n            op(A)Xop(E)' + op(E)Xop(A)' + C = 0,\n\nwhere op(A) = A and op(E) = E if adj = false and op(A) = A' and op(E) = E' if adj = true. The pair (A,E) is in a generalized real or complex Schur form and C is a symmetric or hermitian matrix. The pencil A-λE must not have two eigenvalues α and β such that α+β = 0. The computed symmetric or hermitian solution X is contained in C.\n\n\n\n\n\n","category":"function"},{"location":"lyapunov.html#MatrixEquations.tlyapc","page":"Lyapunov Matrix Equation Solvers","title":"MatrixEquations.tlyapc","text":"X = tlyapc(A,C, isig = 1; fast = true, atol::Real=0, rtol::Real=atol>0 ? 0 : N*ϵ)\n\nCompute for isig = ±1 a solution of the continuous T-Lyapunov matrix equation\n\n            A*X + isig*transpose(X)*transpose(A) + C = 0\n\nusing explicit formulas based on full rank factorizations of A  (see [1] and [2]).  A and C are m×n and m×m matrices, respectively, and X is an n×m matrix. The matrix C must be symmetric if isig = 1 and skew-symmetric if isig = -1. atol and rtol are the absolute and relative tolerances, respectively, used for rank computation.  The default relative tolerance is N*ϵ, where N = min(m,n) and ϵ is the machine precision of  the element type of A.\n\nThe underlying rank revealing factorization of A is the QR-factorization with column pivoting,  if fast = true (default), or the more reliable SVD-decomposition, if fast = false.\n\n[1] H. W. Braden. The equations A^T X ± X^T A = B. SIAM J. Matrix Anal. Appl., 20(2):295–302, 1998.\n\n[2] C.-Y. Chiang, E. K.-W. Chu, W.-W. Lin, On the ★-Sylvester equation AX ± X^★ B^★ = C.     Applied Mathematics and Computation, 218:8393–8407, 2012.\n\n\n\n\n\n","category":"function"},{"location":"lyapunov.html#MatrixEquations.hlyapc","page":"Lyapunov Matrix Equation Solvers","title":"MatrixEquations.hlyapc","text":"X = hlyapc(A,C, isig = 1; atol::Real=0, rtol::Real=atol>0 ? 0 : N*ϵ)\n\nCompute for isig = ±1 a solution of the continuous H-Lyapunov matrix equation\n\n            A*X + isig*adjoint(X)*adjoint(A) + C = 0\n\nusing explicit formulas based on full rank factorizations of A  (see [1] and [2]).  A and C are m×n and m×m matrices, respectively, and X is an n×m matrix. The matrix C must be hermitian if isig = 1 and skew-hermitian if isig = -1. atol and rtol are the absolute and relative tolerances, respectively, used for rank computation.  The default relative tolerance is N*ϵ, where N = min(m,n) and ϵ is the machine precision  of the element type of A.\n\nThe underlying rank revealing factorization of Ae (or of A if real) is the QR-factorization with column pivoting,  if fast = true (default), or the more reliable SVD-decomposition, if fast = false.\n\n[1]  H. W. Braden. The equations A^T X ± X^T A = B. SIAM J. Matrix Anal. Appl., 20(2):295–302, 1998.\n\n[2] C.-Y. Chiang, E. K.-W. Chu, W.-W. Lin, On the ★-Sylvester equation AX ± X^★ B^★ = C.     Applied Mathematics and Computation, 218:8393–8407, 2012.\n\n\n\n\n\n","category":"function"},{"location":"lyapunov.html#MatrixEquations.tulyapc!","page":"Lyapunov Matrix Equation Solvers","title":"MatrixEquations.tulyapc!","text":"X = tulyapc!(U, Q; adj = false)\n\nCompute for an upper triangular U and a symmetric Q an upper triangular solution X of the continuous T-Lyapunov matrix equation\n\n            transpose(U)*X + transpose(X)*U = Q  if adj = false,\n\nor\n\n            U*transpose(X) + X*transpose(U) = Q   if adj = true.\n\nThe solution X overwrites the matrix Q, while U is unchanged.\n\nIf U is nonsingular, a backward elimination method is used, if adj = false, or a forward elimination method is used if adj = true, by adapting the approach  employed in the function dU_from_dQ! of the DiffOpt.jl package.  For a n×n  singular U, a least-squares upper-triangular solution X is determined using a conjugate-gradient based iterative method applied  to a suitably defined T-Lyapunov linear operator L:X -> Y, which maps upper triangular matrices X into upper triangular matrices Y, and the associated matrix M = Matrix(L) is n(n+1)2 times n(n+1)2.  In this case, the keyword argument tol (default: tol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"lyapunov.html#MatrixEquations.hulyapc!","page":"Lyapunov Matrix Equation Solvers","title":"MatrixEquations.hulyapc!","text":"X = hulyapc!(U, Q; adj = false)\n\nCompute for an upper triangular U and a hermitian Q an upper triangular solution X of the continuous H-Lyapunov matrix equation\n\n            U'*X + X'*U = Q   if adj = false,\n\nor\n\n            U*X' + X*U' = Q   if adj = true.\n\nThe solution X overwrites the matrix Q, while U is unchanged.\n\nIf U is nonsingular, a backward elimination method is used, if adj = false, or a forward elimination method is used if adj = true, by adapting the approach  employed in the function dU_from_dQ! of the DiffOpt.jl package.  For a n×n  singular U, a least-squares upper-triangular solution X is determined using a conjugate-gradient based iterative method applied  to a suitably defined T-Lyapunov linear operator L:X -> Y, which maps upper triangular matrices X into upper triangular matrices Y, and the associated matrix M = Matrix(L) is n(n+1)2 times n(n+1)2.  In this case, the keyword argument tol (default: tol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"lyapunov.html#Discrete-time-Lyapunov-(Stein)-Matrix-Equations","page":"Lyapunov Matrix Equation Solvers","title":"Discrete-time Lyapunov (Stein) Matrix Equations","text":"","category":"section"},{"location":"lyapunov.html","page":"Lyapunov Matrix Equation Solvers","title":"Lyapunov Matrix Equation Solvers","text":"lyapd\nlyapds!","category":"page"},{"location":"lyapunov.html#MatrixEquations.lyapd","page":"Lyapunov Matrix Equation Solvers","title":"MatrixEquations.lyapd","text":"X = lyapd(A, C)\n\nCompute X, the solution of the discrete Lyapunov equation\n\n   AXA' - X + C = 0,\n\nwhere A is a square real or complex matrix and C is a square matrix. A must not have two eigenvalues α and β such that αβ = 1. The solution X is symmetric or hermitian if C is a symmetric or hermitian.\n\nThe following particular cases are also adressed:\n\nX = lyapd(α*I,C)  or  X = lyapd(α,C)\n\nSolve the matrix equation (αα'-1)X + C = 0.\n\nx = lyapd(α,γ)\n\nSolve the equation (αα'-1)x + γ = 0.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> C = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyapd(A, C)\n2×2 Array{Float64,2}:\n  0.2375  -0.2125\n -0.2125   0.1375\n\njulia> A*X*A' - X + C\n2×2 Array{Float64,2}:\n 5.55112e-16  6.66134e-16\n 2.22045e-16  4.44089e-16\n\n\n\n\n\nX = lyapd(A, E, C)\n\nCompute X, the solution of the generalized discrete Lyapunov equation\n\n     AXA' - EXE' + C = 0,\n\nwhere A and E are square real or complex matrices and C is a square matrix. The pencil A-λE must not have two eigenvalues α and β such that αβ = 1. The solution X is symmetric or hermitian if C is a symmetric or hermitian.\n\nThe following particular cases are also adressed:\n\nX = lyapd(A,β*I,C)  or  X = lyapd(A,β,C)\n\nSolve the matrix equation AXA' - βXβ' + C = 0.\n\nX = lyapd(α*I,E,C)  or  X = lyapd(α,E,C)\n\nSolve the matrix equation αXα' - EXE' + C = 0.\n\nX = lyapd(α*I,β*I,C)  or  X = lyapd(α,β,C)\n\nSolve the matrix equation (αα'-ββ')X + C = 0.\n\nx = lyapd(α,β,γ)\n\nSolve the equation (αα'-ββ')x + γ = 0.\n\nExample\n\njulia> A = [3. 4.; 5. 6.]\n2×2 Array{Float64,2}:\n 3.0  4.0\n 5.0  6.0\n\njulia> E = [ 1. 2.; 0. -1.]\n2×2 Array{Float64,2}:\n 1.0   2.0\n 0.0  -1.0\n\njulia> C = [1. 1.; 1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyapd(A, E, C)\n2×2 Array{Float64,2}:\n  1.775  -1.225\n -1.225   0.775\n\njulia> A*X*A' - E*X*E' + C\n2×2 Array{Float64,2}:\n -2.22045e-16  -4.44089e-16\n -1.33227e-15   1.11022e-15\n\n\n\n\n\n","category":"function"},{"location":"lyapunov.html#MatrixEquations.lyapds!","page":"Lyapunov Matrix Equation Solvers","title":"MatrixEquations.lyapds!","text":"lyapds!(A, C; adj = false)\n\nSolve the discrete Lyapunov matrix equation\n\n            op(A)Xop(A)' - X + C = 0,\n\nwhere op(A) = A if adj = false and op(A) = A' if adj = true. A is in a real or complex Schur form and C is a symmetric or hermitian matrix. A must not have two eigenvalues α and β such that αβ = 1. The computed symmetric or hermitian solution X is contained in C.\n\n\n\n\n\nlyapds!(A, E, C; adj = false)\n\nSolve the generalized discrete Lyapunov matrix equation\n\n            op(A)Xop(A)' - op(E)Xop(E)' + C = 0,\n\nwhere op(A) = A and op(E) = E if adj = false and op(A) = A' and op(E) = E' if adj = true. The pair (A,E) is in a generalized real or complex Schur form and C is a symmetric or hermitian matrix. The pencil A-λE must not have two eigenvalues α and β such that αβ = 1. The computed symmetric or hermitian solution X is contained in C.\n\n\n\n\n\n","category":"function"},{"location":"makeindex.html#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"makeindex.html","page":"Index","title":"Index","text":"Pages = [\"lyapunov.md\", \"plyapunov.md\", \"riccati.md\", \"sylvester.md\", \"sylvkr.md\", \"condest.md\",\"meoperators.md\", \"lapackutil.md\" ]\nModule = [\"MatrixEquations\"]\nOrder = [:type, :function]","category":"page"},{"location":"lapackutil.html#Lapack-Utilities","page":"Lapack Utilities","title":"Lapack Utilities","text":"","category":"section"},{"location":"lapackutil.html","page":"Lapack Utilities","title":"Lapack Utilities","text":"tgsyl!\nlanv2\nlag2\nladiv\nlacn2!","category":"page"},{"location":"lapackutil.html#MatrixEquations.LapackUtil.tgsyl!","page":"Lapack Utilities","title":"MatrixEquations.LapackUtil.tgsyl!","text":"tgsyl!(A, B, C, D, E, F) -> (C, F, scale)\n\nSolve the Sylvester system of matrix equations\n\n  AX - YB = scale*C\n  DX - YE = scale*F ,\n\nwhere X and Y are unknown matrices, the pairs (A, D), (B, E) and  (C, F) have the same sizes, and the pairs (A, D) and (B, E) are in generalized (real) Schur canonical form, i.e. A, B are upper quasi triangular and D, E are upper triangular. Returns X (overwriting C), Y (overwriting F) and scale.\n\ntgsyl!(trans, A, B, C, D, E, F) -> (C, F, scale)\n\nSolve for trans = 'T' and real matrices or for trans = 'C' and complex matrices,  the (adjoint) Sylvester system of matrix equations\n\n  A'X + D'Y = scale*C\n  XB' + YE' = scale*(-F) .\n\ntgsyl!('N', A, B, C, D, E, F) corresponds to the call tgsyl!(A, B, C, D, E, F).\n\nInterface to the LAPACK subroutines DTGSYL/STGSYL/ZTGSYL/CTGSYL.\n\n\n\n\n\n","category":"function"},{"location":"lapackutil.html#MatrixEquations.LapackUtil.lanv2","page":"Lapack Utilities","title":"MatrixEquations.LapackUtil.lanv2","text":"lanv2(A, B, C, D) -> (RT1R, RT1I, RT2R, RT2I, CS, SN)\n\nCompute the Schur factorization of a real 2-by-2 nonsymmetric matrix [A,B;C,D] in standard form. A, B, C, D are overwritten on output by the corresponding elements of the standardised Schur form. RT1R+im*RT1I and RT2R+im*RT2I are the resulting eigenvalues. CS and SN are the parameters of the rotation matrix. Interface to the LAPACK subroutines DLANV2/SLANV2.\n\n\n\n\n\n","category":"function"},{"location":"lapackutil.html#MatrixEquations.LapackUtil.lag2","page":"Lapack Utilities","title":"MatrixEquations.LapackUtil.lag2","text":"lag2(A, B, SAFMIN) -> (SCALE1, SCALE2, WR1, WR2, WI)\n\nCompute the eigenvalues of a 2-by-2 generalized real eigenvalue problem for the matrix pair (A,B), with scaling as necessary to avoid over-/underflow. SAFMIN is the smallest positive number s.t. 1/SAFMIN does not overflow. If WI = 0, WR1/SCALE1 and WR2/SCALE2 are the resulting real eigenvalues, while if WI <> 0, then (WR1+/-im*WI)/SCALE1 are the resulting complex eigenvalues. Interface to the LAPACK subroutines DLAG2/SLAG2.\n\n\n\n\n\n","category":"function"},{"location":"lapackutil.html#MatrixEquations.LapackUtil.ladiv","page":"Lapack Utilities","title":"MatrixEquations.LapackUtil.ladiv","text":"ladiv(A, B, C, D) -> (P, Q)\n\nPerform the complex division in real arithmetic\n\nP + iQ = displaystylefracA+iBC+iD\n\nby avoiding unnecessary overflow. Interface to the LAPACK subroutines DLADIV/SLADIV.\n\n\n\n\n\n","category":"function"},{"location":"lapackutil.html#MatrixEquations.LapackUtil.lacn2!","page":"Lapack Utilities","title":"MatrixEquations.LapackUtil.lacn2!","text":"lacn2!(V, X, ISGN, EST, KASE, ISAVE ) -> (EST, KASE )\n\nEstimate the 1-norm of a real linear operator A, using reverse communication by applying the operator or its transpose/adjoint to a vector. KASE is a parameter to control the norm evaluation process as follows. On the initial call, KASE should be 0. On an intermediate return, KASE will be 1 or 2, indicating whether the real vector X should be overwritten by A * X  or A' * X at the next call. On the final return, KASE will again be 0 and EST is an estimate (a lower bound) for the 1-norm of A. V is a real work vector, ISGN is an integer work vector and ISAVE is a 3-dimensional integer vector used to save information between the calls. Interface to the LAPACK subroutines DLACN2/SLACN2.\n\n\n\n\n\nlacn2!(V, X, EST, KASE, ISAVE ) -> (EST, KASE )\n\nEstimate the 1-norm of a complex linear operator A, using reverse communication by applying the operator or its adjoint to a vector. KASE is a parameter to control the norm evaluation process as follows. On the initial call, KASE should be 0. On an intermediate return, KASE will be 1 or 2, indicating whether the complex vector X should be overwritten by A * X  or A' * X at the next call. On the final return, KASE will again be 0 and EST is an estimate (a lower bound) for the 1-norm of A. V is a complex work vector and ISAVE is a 3-dimensional integer vector used to save information between the calls. Interface to the LAPACK subroutines ZLACN2/CLACN2.\n\n\n\n\n\n","category":"function"},{"location":"iterative.html#Iterative-Solvers","page":"Iterative Solvers","title":"Iterative Solvers","text":"","category":"section"},{"location":"iterative.html#Conjugate-Gradient-based-Solution-of-Linear-Equations","page":"Iterative Solvers","title":"Conjugate Gradient based Solution of Linear Equations","text":"","category":"section"},{"location":"iterative.html","page":"Iterative Solvers","title":"Iterative Solvers","text":"cgls","category":"page"},{"location":"iterative.html#MatrixEquations.cgls","page":"Iterative Solvers","title":"MatrixEquations.cgls","text":" cgls(A, b; shift, abstol, reltol, maxiter, x0) -> (x, info)\n\nSolve Ax = b or minimize norm(Ax-b) using CGLS, the conjugate gradient method for unsymmetric linear equations and least squares problems.  A can be specified either as a rectangular matrix or as a linear operator, as defined in the LinearMaps package.   It is desirable that eltype(A) == eltype(b), otherwise errors may result or additional allocations may occur in operator-vector products. \n\nThe keyword argument shift specifies a regularization parameter as shift = s. If s = 0 (default), then CGLS is Hestenes and Stiefel's specialized form of the conjugate-gradient method for least-squares problems. If s ≠ 0, the system (A'*A + s*I)*b = A'*b is solved. \n\nAn absolute tolerance abstol and a relative tolerance reltol can be specified for stopping the iterative process (default: abstol = 0, reltol = 1.e-6).\n\nThe maximum number of iterations can be specified using maxiter (default: maxiter = max(size(A),20)).\n\nAn initial guess for the solution can be specified using the keyword argument vector x0 (default: x0 = missing). \n\nThe resulting named tuple info contains (flag, resNE, iter), with convergence related information, as follows: \n\n `info.flag`  - convergence flag with values:  \n                1, if convergence occured; \n                2, if the maximum number of iterations has been reached without convergence;\n                3, if the matrix `A'*A + s*I` seems to be singular or indefinite;\n                4, if instability seems likely meaning `(A'*A + s*I)` indefinite and `norm(x)` decreased;  \n\n `info.resNE` - the relative residual for the normal equations `norm(A'*b - (A'*A + s*I)*x)/norm(A'*b)`;  \n\n `info.iter`  - the iteration number at which `x` was computed.\n\nThis function is a translation of the MATLAB implementation of CGLS, the conjugate gradient method for nonsymmetric linear equations and least squares problems https://web.stanford.edu/group/SOL/software/cgls/.  The author of the code is Michael Saunders, with contributions from Per Christian Hansen, Folkert Bleichrodt and Christopher Fougner.    \n\nNote:  Two alternative solvers lsqr and lsmr, available in the IterativeSolvers package, can also be employed.  For example, the following call to lsqr can be alternatively used:\n\n  using IterativeSolvers\n  lsqr(A, b; kwargs...) -> x[, history]\n\nwhere kwargs contains solver-specific keyword arguments. A similar call to  lsmr can be used.    \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#Continuous-time-Lyapunov-and-Lyapunov-like-Matrix-Equations","page":"Iterative Solvers","title":"Continuous-time Lyapunov and Lyapunov-like Matrix Equations","text":"","category":"section"},{"location":"iterative.html","page":"Iterative Solvers","title":"Iterative Solvers","text":"lyapci\nlyapdi\ntlyapci\nhlyapci\ntulyapci\nhulyapci","category":"page"},{"location":"iterative.html#MatrixEquations.lyapci","page":"Iterative Solvers","title":"MatrixEquations.lyapci","text":"lyapci(A, C; abstol, reltol, maxiter) -> (X,info)\n\nCompute for a square A and a hermitian/symmetric C a solution X of the continuous Lyapunov matrix equation\n\n            A*X + X*A' + C = 0.\n\nA least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\nlyapci(A, E, C; abstol, reltol, maxiter) -> (X,info)\n\nCompute X, the solution of the generalized continuous Lyapunov equation\n\nAXE' + EXA' + C = 0,\n\nwhere A and E are square real or complex matrices and C is a square matrix. A least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.lyapdi","page":"Iterative Solvers","title":"MatrixEquations.lyapdi","text":"lyapdi(A, C; abstol, reltol, maxiter) -> (X,info)\n\nCompute for a square A and a hermitian/symmetric C a solution X of the discrete Lyapunov matrix equation\n\n            AXA' - X + C = 0.\n\nA least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\nlyapdi(A, E, C; abstol, reltol, maxiter) -> (X,info)\n\nCompute X, the solution of the generalized discrete Lyapunov equation\n\nAXA' - EXE' + C = 0,\n\nwhere A and E are square real or complex matrices and C is a square matrix. A least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.tlyapci","page":"Iterative Solvers","title":"MatrixEquations.tlyapci","text":"tlyapci(A, C, isig = +1; adj = false, abstol, reltol, maxiter) -> (X,info)\n\nCompute a solution X of the continuous T-Lyapunov matrix equation\n\n            A*X +isig*transpose(X)*transpose(A) = C   if adj = false,\n\nor\n\n            A*transpose(X) + isig*X*transpose(A) = C   if adj = true,\n\nwhere for isig = 1, C is a symmetric matrix and for isig = -1, C is a skew-symmetric matrix.                     \n\nFor a matrix A, a least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined T-Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.hlyapci","page":"Iterative Solvers","title":"MatrixEquations.hlyapci","text":"hlyapci(A, C, isig = +1; adj = false, abstol, reltol, maxiter) -> (X,info)\n\nCompute a solution X of the continuous H-Lyapunov matrix equation\n\n            A*X + isig*X'*A' = C   if adj = false,\n\nor\n\n            A*X' + isig*X*A' = C   if adj = true,\n\nwhere for isig = 1, C is a hermitian matrix and for isig = -1, C is a skew-hermitian matrix.                     \n\nFor a matrix A, a least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined T-Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution.  The keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.tulyapci","page":"Iterative Solvers","title":"MatrixEquations.tulyapci","text":"tulyapci(U, Q; adj = false, abstol, reltol, maxiter) -> (X,info)\n\nCompute for an upper triangular U and a symmetric Q an upper triangular solution X of the continuous T-Lyapunov matrix equation\n\n  transpose(U)*X + transpose(X)*U = Q   if adj = false,\n\nor \n\n  U*transpose(X) + X*transpose(U) = Q   if adj = true.\n\nFor a n×n upper triangular matrix U, a least-squares upper-triangular solution X is determined using a conjugate-gradient based iterative method applied  to a suitably defined T-Lyapunov linear operator L:X -> Y, which maps upper triangular matrices X into upper triangular matrices Y, and the associated matrix M = Matrix(L) is n(n+1)2 times n(n+1)2.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution.  The keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.hulyapci","page":"Iterative Solvers","title":"MatrixEquations.hulyapci","text":"hulyapci(U, Q; adj = false, abstol, reltol, maxiter) -> (X,info)\n\nCompute for an upper triangular U and a hermitian Q an upper triangular solution X of the continuous H-Lyapunov matrix equation\n\n            U'*X + X'*U = Q   if adj = false,\n\nor\n\n            U*X' + X*U' = Q    if adj = true.\n\nFor a n×n upper triangular matrix U, a least-squares upper-triangular solution X is determined using a conjugate-gradient based iterative method applied  to a suitably defined T-Lyapunov linear operator L:X -> Y, which maps upper triangular matrices X into upper triangular matrices Y, and the associated matrix M = Matrix(L) is n(n+1)2 times n(n+1)2.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution.  The keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000).  \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#Sylvester-and-Sylvester-like-Matrix-Equations","page":"Iterative Solvers","title":"Sylvester and Sylvester-like Matrix Equations","text":"","category":"section"},{"location":"iterative.html","page":"Iterative Solvers","title":"Iterative Solvers","text":"sylvci\nsylvdi\ngsylvi\ngtsylvi\nghsylvi","category":"page"},{"location":"iterative.html#MatrixEquations.sylvci","page":"Iterative Solvers","title":"MatrixEquations.sylvci","text":"X = sylvci(A,B,C)\n\nSolve the continuous Sylvester matrix equation\n\n            AX + XB = C ,\n\nwhere A and B are square matrices. \n\nA least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.sylvdi","page":"Iterative Solvers","title":"MatrixEquations.sylvdi","text":"X = sylvdi(A,B,C)\n\nSolve the discrete Sylvester matrix equation\n\n            AXB + X = C ,\n\nwhere A and B are square matrices. \n\nA least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.gsylvi","page":"Iterative Solvers","title":"MatrixEquations.gsylvi","text":"X = gsylvi(A,B,C,D,E)\n\nSolve the generalized Sylvester matrix equation\n\nAXB + CXD = E ,\n\nwhere A, B, C and D are square matrices. \n\nA least-squares solution X is determined using a conjugate gradient based iterative method applied  to a suitably defined Lyapunov linear operator L:X -> Y such that L(X) = C or norm(L(X) - C) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.gtsylvi","page":"Iterative Solvers","title":"MatrixEquations.gtsylvi","text":" gtsylvi(A, B, C, D, E; mx, nx, abstol, reltol, maxiter) -> (X,info)\n\nCompute a solution X of the generalized T-Sylvester matrix equation\n\n  ∑ A_i*X*B_i + ∑ C_j*transpose(X)*D_j = E,\n\nwhere A_i and C_j are matrices having the same row dimension equal to the row dimension of E and  B_i and D_j are matrices having the same column dimension equal to the column dimension of E.  A_i and B_i are contained in the k-vectors of matrices A and B, respectively, and  C_j and D_j are contained in the l-vectors of matrices C and D, respectively.  Any of the component matrices can be given as an UniformScaling.  The keyword parameters mx and nx can be used to specify the row and column dimensions of X,  if they cannot be inferred from the data.\n\nA least-squares solution X is determined using a conjugate-gradient based iterative method applied  to a suitably defined T-Sylvester linear operator L:X -> Y such that L(X) = E or norm(L(X) - E) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\nNote: For the derivation of the adjoint equation see reference [1], which also served as motivation to implement a general linear matrix equation solver in Julia.  \n\n[1] Uhlig, F., Xu, A.B. Iterative optimal solutions of linear matrix equations for hyperspectral and multispectral image fusing. Calcolo 60, 26 (2023).      https://doi.org/10.1007/s10092-023-00514-8\n\n\n\n\n\n","category":"function"},{"location":"iterative.html#MatrixEquations.ghsylvi","page":"Iterative Solvers","title":"MatrixEquations.ghsylvi","text":" ghsylvi(A, B, C, D, E; mx, nx, abstol, reltol, maxiter) -> (X,info)\n\nCompute a solution X of the generalized H-Sylvester matrix equation\n\n  ∑ A_i*X*B_i + ∑ C_j*X'*D_j = E,\n\nwhere A_i and C_j are matrices having the same row dimension equal to the row dimension of E and  B_i and D_j are matrices having the same column dimension equal to the column dimension of E.  A_i and B_i are contained in the k-vectors of matrices A and B, respectively, and  C_j and D_j are contained in the l-vectors of matrices C and D, respectively.  Any of the component matrices can be given as an UniformScaling.  The keyword parameters mx and nx can be used to specify the row and column dimensions of X,  if they cannot be inferred from the data.\n\nA least-squares solution X is determined using a conjugate-gradient based iterative method applied  to a suitably defined T-Sylvester linear operator L:X -> Y such that L(X) = E or norm(L(X) - E) is minimized.  The keyword arguments abstol (default: abstol = 0) and reltol (default: reltol = sqrt(eps())) can be used to provide the desired tolerance for the accuracy of the computed solution and  the keyword argument maxiter can be used to set the maximum number of iterations (default: maxiter = 1000). \n\nNote: For the derivation of the adjoint equation see reference [1], which also served as motivation to implement a general linear matrix equation solver in Julia.  \n\n[1] Uhlig, F., Xu, A.B. Iterative optimal solutions of linear matrix equations for hyperspectral and multispectral image fusing. Calcolo 60, 26 (2023).      https://doi.org/10.1007/s10092-023-00514-8\n\n\n\n\n\n","category":"function"},{"location":"plyapunov.html#Positive-definite-Lyapunov-Matrix-Equation-Solvers","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"Positive-definite Lyapunov Matrix Equation Solvers","text":"","category":"section"},{"location":"plyapunov.html#Continuous-time-Lyapunov-Matrix-Equations","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"Continuous-time Lyapunov Matrix Equations","text":"","category":"section"},{"location":"plyapunov.html","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"Positive-definite Lyapunov Matrix Equation Solvers","text":"plyapc\nplyapcs!","category":"page"},{"location":"plyapunov.html#MatrixEquations.plyapc","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"MatrixEquations.plyapc","text":"U = plyapc(A, B)\n\nCompute U, the upper triangular factor of the solution X = UU' of the continuous Lyapunov equation\n\n  AX + XA' + BB' = 0,\n\nwhere A is a square real or complex matrix and B is a matrix with the same number of rows as A. A must have only eigenvalues with negative real parts.\n\nU = plyapc(A', B')\n\nCompute U, the upper triangular factor of the solution X = U'U of the continuous Lyapunov equation\n\n  A'X + XA + B'B = 0,\n\nwhere A is a square real or complex matrix and B is a matrix with the same number of columns as A. A must have only eigenvalues with negative real parts.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [-2. 1.;-1. -2.]\n2×2 Array{Float64,2}:\n -2.0   1.0\n -1.0  -2.0\n\njulia> B = [1. 1. ;1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> U = plyapc(A,B)\n2×2 UpperTriangular{Float64,Array{Float64,2}}:\n 0.481812  0.801784\n  ⋅        0.935414\n\njulia> A*U*U'+U*U'*A'+B*B'\n2×2 Array{Float64,2}:\n 0.0          8.88178e-16\n 8.88178e-16  3.55271e-15\n\n\n\n\n\nU = plyapc(A, E, B)\n\nCompute U, the upper triangular factor of the solution X = UU' of the generalized continuous Lyapunov equation\n\n  AXE' + EXA' + BB' = 0,\n\nwhere A and E are square real or complex matrices and B is a matrix with the same number of rows as A. The pencil A - λE must have only eigenvalues with negative real parts.\n\nU = plyapc(A', E', B')\n\nCompute U, the upper triangular factor of the solution X = U'U of the generalized continuous Lyapunov equation\n\n  A'XE + E'XA + B'B = 0,\n\nwhere A and E are square real or complex matrices and B is a matrix with the same number of columns as A. The pencil A - λE must have only eigenvalues with negative real parts.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [-2. 1.;-1. -2.]\n2×2 Array{Float64,2}:\n -2.0   1.0\n -1.0  -2.0\n\njulia> E = [1. 0.; 1. 1.]\n2×2 Array{Float64,2}:\n 1.0  0.0\n 1.0  1.0\n\njulia> B = [1. 1. ;1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> U = plyapc(A,E,B)\n2×2 UpperTriangular{Float64,Array{Float64,2}}:\n 0.408248  0.730297\n  ⋅        0.547723\n\njulia> A*U*U'*E'+E*U*U'*A'+B*B'\n2×2 Array{Float64,2}:\n  0.0          -8.88178e-16\n -1.33227e-15  -2.66454e-15\n\n\n\n\n\n","category":"function"},{"location":"plyapunov.html#MatrixEquations.plyapcs!","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"MatrixEquations.plyapcs!","text":"plyapcs!(A,R;adj = false)\n\nSolve the positive continuous Lyapunov matrix equation\n\n            op(A)X + Xop(A)' + op(R)*op(R)' = 0\n\nfor X = op(U)*op(U)', where op(K) = K if adj = false and op(K) = K' if adj = true. A is a square real matrix in a real Schur form  or a square complex matrix in a complex Schur form and R is an upper triangular matrix. A must have only eigenvalues with negative real parts. R contains on output the solution U.\n\n\n\n\n\nplyapcs!(A,E,R;adj = false)\n\nSolve the generalized positive continuous Lyapunov matrix equation\n\n            op(A)Xop(E)' + op(E)*Xop(A)' + op(R)*op(R)' = 0\n\nfor X = op(U)*op(U)', where op(K) = K if adj = false and op(K) = K' if adj = true. The pair (A,E) is in a generalized real/complex Schur form and R is an upper triangular matrix. The pencil A-λE must have only eigenvalues with negative real parts. R contains on output the solution U.\n\n\n\n\n\n","category":"function"},{"location":"plyapunov.html#Discrete-time-Lyapunov-(Stein)-Matrix-Equations","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"Discrete-time Lyapunov (Stein) Matrix Equations","text":"","category":"section"},{"location":"plyapunov.html","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"Positive-definite Lyapunov Matrix Equation Solvers","text":"plyapd\nplyapds!","category":"page"},{"location":"plyapunov.html#MatrixEquations.plyapd","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"MatrixEquations.plyapd","text":"U = plyapd(A, B)\n\nCompute U, the upper triangular factor of the solution X = UU' of the discrete Lyapunov equation\n\n  AXA' - X + BB' = 0,\n\nwhere A is a square real or complex matrix and B is a matrix with the same number of rows as A. A must have only eigenvalues with moduli less than one.\n\nU = plyapd(A', B')\n\nCompute U, the upper triangular factor of the solution X = U'U of the discrete Lyapunov equation\n\n  A'XA - X + B'B = 0,\n\nwhere A is a square real or complex matrix and B is a matrix with the same number of columns as A. A must have only eigenvalues with moduli less than one.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [-0.5 .1;-0.1 -0.5]\n2×2 Array{Float64,2}:\n -0.5   0.1\n -0.1  -0.5\n\njulia> B = [1. 1. ;1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> U = plyapd(A,B)\n2×2 UpperTriangular{Float64,Array{Float64,2}}:\n 0.670145  1.35277\n  ⋅        2.67962\n\njulia> A*U*U'*A'-U*U'+B*B'\n2×2 Array{Float64,2}:\n -4.44089e-16  4.44089e-16\n  4.44089e-16  1.77636e-15\n\n\n\n\n\nU = plyapd(A, E, B)\n\nCompute U, the upper triangular factor of the solution X = UU' of the generalized discrete Lyapunov equation\n\n  AXA' - EXE' + BB' = 0,\n\nwhere A and E are square real or complex matrices and B is a matrix with the same number of rows as A. The pencil A - λE must have only eigenvalues with moduli less than one.\n\nU = plyapd(A', E', B')\n\nCompute U, the upper triangular factor of the solution X = U'U of the generalized discrete Lyapunov equation\n\n  A'XA - E'XE + B'B = 0,\n\nwhere A and E are square real or complex matrices and B is a matrix with the same number of columns as A. The pencil A - λE must have only eigenvalues with moduli less than one.\n\nExample\n\njulia> using LinearAlgebra\n\njulia> A = [-0.5 .1;-0.1 -0.5]\n2×2 Array{Float64,2}:\n -0.5   0.1\n -0.1  -0.5\n\njulia> E = [1. 0.; 1. 1.]\n2×2 Array{Float64,2}:\n 1.0  0.0\n 1.0  1.0\n\njulia> B = [1. 1. ;1. 2.]\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  2.0\n\njulia> U = plyapd(A,E,B)\n2×2 UpperTriangular{Float64,Array{Float64,2}}:\n 1.56276  0.416976\n  ⋅       1.34062\n\njulia> A*U*U'*A'-E*U*U'*E'+B*B'\n2×2 Array{Float64,2}:\n 1.77636e-15  2.22045e-15\n 2.22045e-15  2.66454e-15\n\n\n\n\n\n","category":"function"},{"location":"plyapunov.html#MatrixEquations.plyapds!","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"MatrixEquations.plyapds!","text":"plyapds!(A, R; adj = false)\n\nSolve the positive discrete Lyapunov matrix equation\n\n            op(A)Xop(A)' - X + op(R)*op(R)' = 0\n\nfor X = op(U)*op(U)', where op(K) = K if adj = false and op(K) = K' if adj = true. A is a square real matrix in a real Schur form or a square complex matrix in a complex Schur form and R is an upper triangular matrix. A must have only eigenvalues with moduli less than one. R contains on output the upper triangular solution U.\n\n\n\n\n\nplyapds!(A,E,R;adj = false)\n\nSolve the generalized positive discrete Lyapunov matrix equation\n\n            op(A)Xop(A)' - op(E)Xop(E)' + op(R)*op(R)' = 0\n\nfor X = op(U)*op(U)', where op(K) = K if adj = false and op(K) = K' if adj = true. The pair (A,E) of square real or complex matrices is in a generalized Schur form and R is an upper triangular matrix. A-λE must have only eigenvalues with moduli less than one. R contains on output the upper triangular solution U.\n\n\n\n\n\n","category":"function"},{"location":"plyapunov.html#Schur-Form-Based-Solvers","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"Schur Form Based Solvers","text":"","category":"section"},{"location":"plyapunov.html","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"Positive-definite Lyapunov Matrix Equation Solvers","text":"plyaps","category":"page"},{"location":"plyapunov.html#MatrixEquations.plyaps","page":"Positive-definite Lyapunov Matrix Equation Solvers","title":"MatrixEquations.plyaps","text":"U = plyaps(A, B; disc = false)\n\nCompute U, the upper triangular factor of the solution X = UU' of the continuous Lyapunov equation\n\n  AX + XA' + BB' = 0,\n\nwhere A is a square real or complex matrix in a real or complex Schur form, respectively, and B is a matrix with the same number of rows as A. A must have only eigenvalues with negative real parts. Only the upper Hessenberg part of A is referenced.\n\nU = plyaps(A', B', disc = false)\n\nCompute U, the upper triangular factor of the solution X = U'U of the continuous Lyapunov equation\n\n  A'X + XA + B'B = 0,\n\nwhere A is a square real or complex matrix in a real or complex Schur form, respectively, and B is a matrix with the same number of columns as A. A must have only eigenvalues with negative real parts. Only the upper Hessenberg part of A is referenced.\n\nU = plyaps(A, B, disc = true)\n\nCompute U, the upper triangular factor of the solution X = UU' of the discrete Lyapunov equation\n\n  AXA' - X + BB' = 0,\n\nwhere A is a square real or complex matrix in a real or complex Schur form, respectively, and B is a matrix with the same number of rows as A. A must have only eigenvalues with moduli less than one. Only the upper Hessenberg part of A is referenced.\n\nU = plyaps(A', B', disc = true)\n\nCompute U, the upper triangular factor of the solution X = U'U of the discrete Lyapunov equation\n\n  A'XA - X + B'B = 0,\n\nwhere A is a square real or complex matrix in a real or complex Schur form, respectively, and B is a matrix with the same number of columns as A. A must have only eigenvalues with moduli less than one. Only the upper Hessenberg part of A is referenced.\n\n\n\n\n\nU = plyaps(A, E, B; disc = false)\n\nCompute U, the upper triangular factor of the solution X = UU' of the generalized continuous Lyapunov equation\n\n  AXE' + EXA' + BB' = 0,\n\nwhere A and E are square real or complex matrices with the pair (A,E) in a generalied real or complex Schur form, respectively,  and B is a matrix with the same number of rows as A. The pencil A - λE must have only eigenvalues with negative real parts.\n\nU = plyaps(A', E', B', disc = false)\n\nCompute U, the upper triangular factor of the solution X = U'U of the generalized continuous Lyapunov equation\n\n  A'XE + E'XA + B'B = 0,\n\nwhere A and E are square real or complex matrices with the pair (A,E) in a generalied real or complex Schur form, respectively,  and B is a matrix with the same number of columns as A. The pencil A - λE must have only eigenvalues with negative real parts.\n\nU = plyaps(A, E, B, disc = true)\n\nCompute U, the upper triangular factor of the solution X = UU' of the generalized discrete Lyapunov equation\n\n  AXA' - EXE' + BB' = 0,\n\nwhere A and E are square real or complex matrices with the pair (A,E) in a generalied real or complex Schur form, respectively,  and B is a matrix with the same number of rows as A. The pencil A - λE must have only eigenvalues with moduli less than one.\n\nU = plyaps(A', E', B', disc = true)\n\nCompute U, the upper triangular factor of the solution X = U'U of the generalized discrete Lyapunov equation\n\n  A'XA - E'XE + B'B = 0,\n\nwhere A and E are square real or complex matrices with the pair (A,E) in a generalied real or complex Schur form, respectively,  and B is a matrix with the same number of columns as A. The pencil A - λE must have only eigenvalues with moduli less than one.\n\n\n\n\n\n","category":"function"},{"location":"index.html","page":"Overview","title":"Overview","text":"CurrentModule = MatrixEquations\nDocTestSetup = quote\n    using MatrixEquations\nend","category":"page"},{"location":"index.html#MatrixEquations.jl","page":"Overview","title":"MatrixEquations.jl","text":"","category":"section"},{"location":"index.html","page":"Overview","title":"Overview","text":"(Image: DocBuild) (Image: Code on Github.)","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"This collection of Julia functions is an attemp to implement high performance numerical software to solve classes of Lyapunov, Sylvester and Riccati matrix equations at a performance level comparable with efficient structure exploiting Fortran implementations, as those available in the Systems and Control Library SLICOT. This goal has been fully achieved for Lyapunov and Sylvester equation solvers, for which the codes for both real and complex data perform at practically same performance level as similar functions available in the MATLAB Control System Toolbox (which rely on SLICOT).","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"The available functions in the MatrixEquations.jl package cover both standard and generalized continuous and discrete Lyapunov, Sylvester and Riccati equations for both real and complex data. The functions for the solution of Lyapunov and Sylvester equations rely on efficient structure exploiting solvers for which the input data are in Schur or generalized Schur forms. A comprehensive set of Lyapunov and Sylvester operators has been implemented, which allow the estimation of condition numbers of these operators and the iterative solution of various Lyapunov and Sylvester matrix equations using the conjugate gradient method. The implementation of Riccati equation solvers employ orthogonal Schur vectors based methods and their extensions to linear matrix pencil based reduction approaches. The calls of all functions with adjoint (in complex case) or transposed (in real case) arguments are fully supported by appropriate computational algorithms, thus the matrix copying operations are mostly avoided.","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"The current version of the package includes the following functions:","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Solution of Lyapunov equations","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Function Description\nlyapc Solution of the continuous Lyapunov equations\nlyapd Solution of the discrete Lyapunov equations\ntlyapc Solution of the continuous T-Lyapunov equations\nhlyapc Solution of the continuous H-Lyapunov equations\ntulyapc! Computation of the upper triangular solution of the continuous T-Lyapunov equation\nhulyapc! Computation of the upper triangular solution of the continuous H-Lyapunov equation\nplyapc Solution of the positive continuous Lyapunov equations\nplyapd Solution of the positive discrete Lyapunov equations","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Solution of algebraic  Riccati equations","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Function Description\narec Solution of the continuous Riccati equations\ngarec Solution of the generalized continuous Riccati equation\nared Solution of the discrete Riccati equation\ngared Solution of the generalized discrete Riccati equation","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Solution of Sylvester equations and systems","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Function Description\nsylvc Solution of the (continuous) Sylvester equations\nsylvd Solution of the (discrete) Sylvester equations\ngsylv Solution of the generalized Sylvester equations\nsylvsys Solution of the Sylvester system of matrix equations\ndsylvsys Solution of the dual Sylvester system of matrix equations","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Iterative solution of linear matrix equations","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Function Description\nlyapci Iterative solution of continuous Lyapunov equations\nlyapdi Iterative solution of discrete Lyapunov equations\ntlyapci Iterative solution of the continuous T-Lyapunov equations\nhlyapci Iterative solution of the continuous H-Lyapunov equations\ntulyapci Iterative solution of the continuous T-Lyapunov equations with upper triangular solutions\nhulyapci Iterative solution of the continuous H-Lyapunov equations with upper triangular solutions\nsylvci Iterative solution of the (continuous) Sylvester equations\nsylvdi Iterative solution of the (discrete) Sylvester equations\ngsylvi Iterative solution of the generalized Sylvester equations\ngtsylvi Iterative solution of the generalized T-Sylvester equations\nghsylvi Iterative solution of the generalized H-Sylvester equations\ncgls The conjugate gradient method for nonsymmetric linear equations and least squares problems","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Norm, condition and separation estimation of linear operators","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Function Description\nopnorm1 Computation of the 1-norm of a linear operator\nopnorm1est Estimation of the 1-norm of a linear operator\noprcondest Estimation of the reciprocal 1-norm condition number of an operator\nopsepest Estimation of the separation of a linear operator","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"The general solvers of Lyapunov and Sylvester equations rely on a set of specialized solvers for real or complex matrices in appropriate Schur forms. For testing purposes, a set of solvers for Sylvester equations has been implemented, which employ the Kronecker-product expansion of the equations. These solvers are not recommended for large order matrices. Based on the conjugate gradient method to solve linear systems or least-squares problems, several iterative solvers have been implemented, which are potentially applicable to solve linear matrix equations with large order dense and sparse matrices. The norms, reciprocal condition numbers and separations can be estimated for a comprehensive set of predefined Lyapunov and Sylvester operators. A complete list of implemented functions is available here.","category":"page"},{"location":"index.html#Future-plans","page":"Overview","title":"Future plans","text":"","category":"section"},{"location":"index.html","page":"Overview","title":"Overview","text":"The collection of tools can be extended by adding new functionality, such as expert solvers, which additionally compute error bounds and condition estimates, or solvers for new classes of Sylvester-like equations or Riccati equations (as those arising in game-theoretic optimization problems). Further performance improvements are still possible by employing blocking based variants of solvers for Lyapunov and Sylvester equations.","category":"page"},{"location":"index.html#[Release-Notes](https://github.com/andreasvarga/MatrixEquations.jl/blob/master/ReleaseNotes.md)","page":"Overview","title":"Release Notes","text":"","category":"section"},{"location":"index.html#Main-developer","page":"Overview","title":"Main developer","text":"","category":"section"},{"location":"index.html","page":"Overview","title":"Overview","text":"Andreas Varga","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"License: MIT (expat)","category":"page"},{"location":"condest.html#Norm,-Condition-Number-and-Separation-Estimations","page":"Norm, Condition Number and Separation Estimations","title":"Norm, Condition Number and Separation Estimations","text":"","category":"section"},{"location":"condest.html","page":"Norm, Condition Number and Separation Estimations","title":"Norm, Condition Number and Separation Estimations","text":"opnorm1\nopnorm1est\noprcondest\nopsepest","category":"page"},{"location":"condest.html#MatrixEquations.opnorm1","page":"Norm, Condition Number and Separation Estimations","title":"MatrixEquations.opnorm1","text":"γ = opnorm1(op)\n\nCompute γ, the induced 1-norm of the linear operator op, as the maximum of 1-norm of the columns of the associated m x n matrix M = Matrix(op):\n\ngamma = op_1 = max_1  j  n M_j_1\n\nwith M_j the j-th column of M. This function is not recommended to be used for large order operators.\n\nExamples\n\njulia> A = [-6. -2. 1.; 5. 1. -1; -4. -2. -1.]\n3×3 Array{Float64,2}:\n -6.0  -2.0   1.0\n  5.0   1.0  -1.0\n -4.0  -2.0  -1.0\n\njulia> opnorm1(lyapop(A))\n30.0\n\njulia> opnorm1(invlyapop(A))\n3.7666666666666706\n\n\n\n\n\n","category":"function"},{"location":"condest.html#MatrixEquations.opnorm1est","page":"Norm, Condition Number and Separation Estimations","title":"MatrixEquations.opnorm1est","text":"γ = opnorm1est(op)\n\nCompute γ, a lower bound of the 1-norm of the square linear operator op, using reverse communication based computations to evaluate op * x and op' * x. It is expected that in most cases gamma  op_110, which is usually acceptable for estimating the condition numbers of linear operators.\n\nExamples\n\njulia> A = [-6. -2. 1.; 5. 1. -1; -4. -2. -1.]\n3×3 Array{Float64,2}:\n -6.0  -2.0   1.0\n  5.0   1.0  -1.0\n -4.0  -2.0  -1.0\n\njulia> opnorm1est(lyapop(A))\n18.0\n\njulia> opnorm1est(invlyapop(A))\n3.76666666666667\n\n\n\n\n\n","category":"function"},{"location":"condest.html#MatrixEquations.oprcondest","page":"Norm, Condition Number and Separation Estimations","title":"MatrixEquations.oprcondest","text":"rcond = oprcondest(op, opinv; exact = false)\n\nCompute rcond, an estimation of the 1-norm reciprocal condition number of a linear operator op, where opinv is the inverse operator inv(op). The estimate is computed as textrcond = 1  (op_1opinv_1), using estimates of the 1-norm, if exact = false, or computed exact values of the 1-norm, if exact = true. The exact = true option is not recommended for large order operators.\n\nNote: No check is performed to verify that opinv = inv(op).\n\nExamples\n\njulia> A = [-6. -2. 1.; 5. 1. -1; -4. -2. -1.]\n3×3 Array{Float64,2}:\n -6.0  -2.0   1.0\n  5.0   1.0  -1.0\n -4.0  -2.0  -1.0\n\njulia> oprcondest(lyapop(A),invlyapop(A))\n0.014749262536873142\n\njulia> 1/opnorm1est(lyapop(A))/opnorm1est(invlyapop(A))\n0.014749262536873142\n\njulia> oprcondest(lyapop(A),invlyapop(A),exact = true)\n0.008849557522123885\n\njulia> 1/opnorm1(lyapop(A))/opnorm1(invlyapop(A))\n0.008849557522123885\n\n\n\n\n\nrcond = oprcondest(op; exact = false)\n\nCompute rcond, an estimation of the 1-norm reciprocal condition number of a linear operator op, where op is one of the defined Lyapunov or Sylvester operators. The estimate is computed as textrcond = 1  (op_1inv(op)_1), using estimates of the 1-norm, if exact = false, or computed exact values of the 1-norm, if exact = true. The exact = true option is not recommended for large order operators.\n\n\n\n\n\n","category":"function"},{"location":"condest.html#MatrixEquations.opsepest","page":"Norm, Condition Number and Separation Estimations","title":"MatrixEquations.opsepest","text":"sep = opsepest(opinv; exact = false)\n\nCompute sep, an estimation of the 1-norm separation of a linear operator op, where opinv is the inverse operator inv(op). The estimate is computed as textsep  = 1  opinv_1 , using an estimate of the 1-norm, if exact = false, or the computed exact value of the 1-norm, if exact = true. The exact = true option is not recommended for large order operators.\n\nThe separation of the operator op is defined as\n\ntextsep = displaystylemin_Xneq 0 fracop*XX\n\nAn estimate of the reciprocal condition number of op can be computed as textsepop_1.\n\nExample\n\njulia> A = [-6. -2. 1.; 5. 1. -1; -4. -2. -1.]\n3×3 Array{Float64,2}:\n -6.0  -2.0   1.0\n  5.0   1.0  -1.0\n -4.0  -2.0  -1.0\n\njulia> opsepest(invlyapop(A))\n0.26548672566371656\n\njulia> 1/opnorm1est(invlyapop(A))\n0.26548672566371656\n\njulia> opsepest(invlyapop(A),exact = true)\n0.26548672566371656\n\njulia> 1/opnorm1(invlyapop(A))\n0.26548672566371656\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#Matrix-Equation-Solvers-using-Kronecker-product-Expansions","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"Matrix Equation Solvers using Kronecker-product Expansions","text":"","category":"section"},{"location":"sylvkr.html","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"Matrix Equation Solvers using Kronecker-product Expansions","text":"sylvckr\nsylvdkr\ngsylvkr\nsylvsyskr\ndsylvsyskr\ntlyapckr\nhlyapckr\ntsylvckr\nhsylvckr\ncsylvckr\ntsylvdkr\nhsylvdkr\ncsylvdkr","category":"page"},{"location":"sylvkr.html#MatrixEquations.sylvckr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.sylvckr","text":"X = sylvckr(A,B,C)\n\nSolve the continuous Sylvester matrix equation\n\n            AX + XB = C\n\nusing the Kronecker product expansion of equations. A and B are square matrices, and A and -B must not have common eigenvalues. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.sylvdkr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.sylvdkr","text":"X = sylvdkr(A,B,C)\n\nSolve the discrete Sylvester matrix equation\n\n            AXB + X = C\n\nusing the Kronecker product expansion of equations. A and B are square matrices, and A and -B must not have common reciprocal eigenvalues. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.gsylvkr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.gsylvkr","text":"X = gsylvkr(A,B,C,D,E)\n\nSolve the generalized Sylvester matrix equation\n\n            AXB + CXD = E\n\nusing the Kronecker product expansion of equations. A, B, C and D are square matrices. The pencils A-λC and D+λB must be regular and must not have common eigenvalues. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.sylvsyskr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.sylvsyskr","text":"sylvsyskr(A,B,C,D,E,F) -> (X,Y)\n\nSolve the Sylvester system of matrix equations\n\n            AX + YB = C\n            DX + YE = F\n\nusing the Kronecker product expansion of equations. (A,D), (B,E) are pairs of square matrices of the same size. The pencils A-λD and -B+λE must be regular and must not have common eigenvalues. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.dsylvsyskr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.dsylvsyskr","text":"dsylvsyskr(A,B,C,D,E,F) -> (X,Y)\n\nSolve the dual Sylvester system of matrix equations\n\n   AX + DY = C\n   XB + YE = F\n\nusing the Kronecker product expansion of equations. (A,D), (B,E) are pairs of square matrices of the same size. The pencils A-λD and -B+λE must be regular and must not have common eigenvalues. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.tlyapckr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.tlyapckr","text":"X = tlyapckr(A,C, isig = 1; atol::Real=0, rtol::Real=atol>0 ? 0 : N*ϵ)\n\nCompute for isig = ±1 a solution of the the continuous T-Lyapunov matrix equation\n\n            A*X + isig*transpose(X)*transpose(A) + C = 0\n\nusing the Kronecker product expansion of equations. A and C are m×n and m×m matrices, respectively, and X is an n×m matrix. The matrix C must be symmetric if isig = 1 and skew-symmetric if isig = -1. atol and rtol are the absolute and relative tolerances, respectively, used for rank computation.  The default relative tolerance is N*ϵ,   where N = 4*min(m,n)^2 and ϵ is the machine precision of the element type of A. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.hlyapckr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.hlyapckr","text":"X = hlyapckr(A,C, isig = 1; atol::Real=0, rtol::Real=atol>0 ? 0 : N*ϵ)\n\nCompute for isig = ±1 a solution of the continuous H-Lyapunov matrix equation\n\n            A*X + isig*adjoint(X)*adjoint(A) + C = 0\n\nusing the Kronecker product expansion of equations. A and C are m×n and m×m matrices, respectively, and X is an n×m matrix. The matrix C must be hermitian if isig = 1 and skew-hermitian if isig = -1. atol and rtol are the absolute and relative tolerances, respectively, used for rank computation.  The default relative tolerance is N*ϵ,   where N = 4*min(m,n)^2 and ϵ is the machine precision of the element type of A. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.tsylvckr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.tsylvckr","text":"X = tsylvckr(A,B,C; atol::Real=0, rtol::Real=atol>0 ? 0 : N*ϵ)\n\nCompute a solution of the continuous T-Sylvester matrix equation\n\n            A*X + transpose(X)*B = C\n\nusing the Kronecker product expansion of equations. A, B and C are m×n, n×m and m×m matrices, respectively, and X is an n×m matrix. atol and rtol are the absolute and relative tolerances, respectively, used for rank computation.  The default relative tolerance is N*ϵ,   where N = 4*min(m,n)^2 and ϵ is the machine precision of the element type of A. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.hsylvckr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.hsylvckr","text":"X = hsylvckr(A,B,C; atol::Real=0, rtol::Real=atol>0 ? 0 : N*ϵ)\n\nCompute a solution of the continuous H-Sylvester matrix equation\n\n            A*X + adjoint(X)*B = C\n\nusing the Kronecker product expansion of equations. A, B and C are m×n, n×m and m×m matrices, respectively, and X is an n×m matrix. atol and rtol are the absolute and relative tolerances, respectively, used for rank computation.  The default relative tolerance is N*ϵ,   where N = 4*min(m,n)^2 and ϵ is the machine precision of the element type of A. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.csylvckr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.csylvckr","text":"X = csylvckr(A,B,C)\n\nSolve the continuous C-Sylvester matrix equation\n\n            A*X + conj(X)*B = C\n\nusing the Kronecker product expansion of equations. A, B and C are m×m, n×n and m×n matrices, respectively, and X is an m×n matrix. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.tsylvdkr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.tsylvdkr","text":"X = tsylvdkr(A,B,C)\n\nSolve the discrete T-Sylvester matrix equation\n\n            A*transpose(X)*B + X = C\n\nusing the Kronecker product expansion of equations. A, B and C are m×n matrices and X is an m×n matrix. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.hsylvdkr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.hsylvdkr","text":"X = hsylvdkr(A,B,C)\n\nSolve the discrete H-Sylvester matrix equation\n\n            A*adjoint(X)*B + X = C\n\nusing the Kronecker product expansion of equations. A, B and C are m×n matrices and X is an m×n matrix. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"},{"location":"sylvkr.html#MatrixEquations.csylvdkr","page":"Matrix Equation Solvers using Kronecker-product Expansions","title":"MatrixEquations.csylvdkr","text":"X = csylvdkr(A,B,C)\n\nSolve the discrete C-Sylvester matrix equation\n\n            A*conj(X)*B + X = C\n\nusing the Kronecker product expansion of equations. A, B and C are m×m, n×n and m×n matrices, respectively, and X is an m×n matrix. This function is not recommended for large order matrices.\n\n\n\n\n\n","category":"function"}]
}
